---
title: "Web scraping<br>many pages"
subtitle: "Lecture 14"
date: "2025-10-09"
format: 
  revealjs: 
    output-file: 14-data-science-ethics-slides.html
    footer: "[üîó sta199-f25.github.io](https://sta199-f25.github.io/)"
    theme: slides.scss
    transition: fade
    slide-number: true
    logo: images/logo.png
    pdf-separate-fragments: true
    toc: false
  html: 
    code-link: true
filters: 
  - ../remove-fmt-skip.lua
editor_options: 
  chunk_output_type: console
---

```{r}
#| label: setup
#| include: false
ggplot2::theme_set(ggplot2::theme_gray(base_size = 24))
todays_ae <- "ae-09-chronicle-scrape"
```

# Warm-up

## While you wait: Participate üì±üíª {.xsmall}

::: columns

::: {.column width="70%"}

::: wooclap

Put the folllowing tasks in order to scrape data from a website:

::: wooclap-choices
- Use the SelectorGadget identify tags for elements you want to grab
- Use `read_html()` to read the page's source code into R
- Use other functions from the **rvest** package to parse the elements you're interested in
- Put the components together in a data frame (a tibble) and analyze it like you analyze any other data
:::

:::

:::

::: {.column width="30%"}
{{< include _wooclap-column.qmd >}}
:::

:::

## Announcements {.smaller}

- ...

# From last time

## A new R workflow {.smaller}

-   When working in a Quarto document, your analysis is re-run each time you knit

-   If web scraping in a Quarto document, you'd be re-scraping the data each time you knit, which is undesirable (and not *nice*)!

-   An alternative workflow:

    -   Use an R script to save your code
    -   Saving interim data scraped using the code in the script as CSV or RDS files
    -   Use the saved data in your analysis in your Quarto document


# Scraping from many pages

## Packages

```{r}
#| message: false
library(tidyverse)
library(rvest) # for web scraping
library(knitr) # for pretty tables
```

## Columns and words

::: question
What additional information do we need to produce the table below?
:::

```{r}
#| include: false
chronicle_article <- read_csv("data/chronicle-article.csv")
```

```{r}
#| label: chronicle-article-word-counts
#| echo: false
chronicle_article |>
  mutate(n_words = str_count(article, " ") + 1) |>
  group_by(column) |>
  summarize(avg_n_words = mean(n_words), n_articles = n()) |>
  knitr::kable(
    col.names = c("Column", "Avg. # words/article", "# articles"),
    digits = 0
  )
```

## Start with the URL for an first article

```{r}
#| message: false
chronicle <- read_csv("data/chronicle.csv")

article_url <- chronicle |>
  slice_head(n = 1) |>
  pull()

article_url
```

## Read in the page

```{r}
article_page <- read_html(article_url)

article_page
```

## Identify the elements you want

```
.article-content
```

![](images/14/article-content.png){fig-align="center"}

## Parse the elements you want - 1 {.smaller}

```{r}
#| eval: false
article_page |>
  html_elements(".article-content")
```

```{r}
#| echo: false
options(width = 500)
article_page |>
  html_elements(".article-content")
```

## Parse the elements you want - 2 {.smaller}

```{r}
article_page |>
  html_elements(".article-content") |>
  html_text2()
```

## Parse the elements you want - 3 {.smaller}

```{r}
article_page |>
  html_elements(".article-content") |>
  html_text2() |>
  str_remove_all("\n")
```

## Wrap in a function

```{r}
#| code-line-numbers: "|1|2|4-7"
parse_article_page <- function(url) {
  article_page <- read_html(url)

  article_page |>
    html_elements(".article-content") |>
    html_text2() |>
    str_remove_all("\n")
}
```

## Functions in R {.smaller}

```{r}
#| eval: false
function_name <- function(argument_1, argument_2, ...) {
  # what the function does
}
```

. . .

For example:

```{r}
multiply_by_3 <- function(x) {
  3 * x
}
```

<br>

. . .

```{r}
multiply_by_3(2)
multiply_by_3(10)
```

## Revisit: `parse_article_page()` {.smaller}

```{r}
#| code-line-numbers: "|1|2|4|5|6|7"
# fmt: skip
parse_article_page <- function(url) {    # define a function with one argument, url
  article_page <- read_html(url)         # read in the page at the url

  article_page |> # start with the page
    html_elements(".article-content") |> # extract element w/ selector .article-content
    html_text2() |>                      # extract text from element (and clean it up)
    str_remove_all("\n")                 # remove all newline characters, return result
}
```

## Test the function {.smaller}

```{r}
#| eval: false
chronicle |>
  slice_head(n = 1) |>
  mutate(article = parse_article_page(url)) |>
  select(title, article)
```

```{r}
#| echo: false
options(width = 200)
chronicle |>
  slice_head(n = 1) |>
  mutate(article = parse_article_page(url)) |>
  select(title, article)
```

## Test the function {.smaller}

```{r}
#| eval: false
#| code-line-numbers: "|3"
chronicle |>
  slice_head(n = 3) |>
  rowwise() |>
  mutate(article = parse_article_page(url)) |>
  select(title, article)
```

```{r}
#| echo: false
options(width = 200)
chronicle |>
  slice_head(n = 3) |>
  rowwise() |>
  mutate(article = parse_article_page(url)) |>
  select(title, article)
```

## Test the function {.smaller}

```{r}
#| eval: false
#| code-line-numbers: "|5"
chronicle |>
  slice_head(n = 3) |>
  rowwise() |>
  mutate(article = parse_article_page(url)) |>
  ungroup() |>
  select(title, article)
```

```{r}
#| echo: false
options(width = 200)
chronicle |>
  slice_head(n = 3) |>
  rowwise() |>
  mutate(article = parse_article_page(url)) |>
  ungroup() |>
  select(title, article)
```

## All articles {.smaller}

This can take a bit to run!

```{r}
#| eval: false
chronicle_article <- chronicle |>
  rowwise() |>
  mutate(article = parse_article_page(url)) |>
  ungroup()

chronicle_article
```

```{r}
#| echo: false
chronicle_article
```

## Revisit: Summary table

::: question
Now that you have the data, how would you produce the summary table below?
:::

```{r}
#| ref-label: chronicle-article-word-counts
#| echo: false
```

## Summarize 1 {.smaller}

```{r}
#| code-line-numbers: "|2"
chronicle_article |>
  mutate(n_words = str_count(article, " ") + 1) |>
  relocate(n_words)
```

## Summarize 2 {.smaller}

```{r}
#| code-line-numbers: "|3"
chronicle_article |>
  mutate(n_words = str_count(article, " ") + 1) |>
  group_by(column)
```

## Summarize 3 {.smaller}

```{r}
#| code-line-numbers: "|4-6"
chronicle_article |>
  mutate(n_words = str_count(article, " ") + 1) |>
  group_by(column) |>
  summarize(
    avg_n_words = mean(n_words)
  )
```

## Summarize 4 {.smaller}

```{r}
#| code-line-numbers: "|4-7"
chronicle_article |>
  mutate(n_words = str_count(article, " ") + 1) |>
  group_by(column) |>
  summarize(
    avg_n_words = mean(n_words),
    n_articles = n()
  )
```

## Make a pretty table {.smaller}

with the `kable()` function from the `knitr` package:

```{r}
#| code-line-numbers: "|8"
chronicle_article |>
  mutate(n_words = str_count(article, " ") + 1) |>
  group_by(column) |>
  summarize(
    avg_n_words = mean(n_words),
    n_articles = n()
  ) |>
  kable()
```

## Update column names {.smaller}

```{r}
#| code-line-numbers: "|9"
chronicle_article |>
  mutate(n_words = str_count(article, " ") + 1) |>
  group_by(column) |>
  summarize(
    avg_n_words = mean(n_words),
    n_articles = n()
  ) |>
  kable(
    col.names = c("Column", "Avg. # words/article", "# articles")
  )
```

## Update digits {.smaller}

```{r}
#| code-line-numbers: "|10"
chronicle_article |>
  mutate(n_words = str_count(article, " ") + 1) |>
  group_by(column) |>
  summarize(
    avg_n_words = mean(n_words),
    n_articles = n()
  ) |>
  kable(
    col.names = c("Column", "Avg. # words/article", "# articles"),
    digits = 0
  )
```

# Web scraping considerations

## Challenges: Unreliable formatting

![](images/14/unreliable-formatting.png){fig-align="center" width="699"}

## Challenges: Data broken into many pages

![](images/14/many-pages.png){fig-align="center"}

## Ethics: "Can you?" vs "Should you?" {.smaller}

![](images/14/ok-cupid-1.png){fig-align="center" width="700"}

::: aside
Source: Brian Resnick, [Researchers just released profile data on 70,000 OkCupid users without permission](https://www.vox.com/2016/5/14/11666116/70000-okcupid-users-data-release), Vox.
:::

# Privacy

## "Your" data

-   Every time we use apps, websites, and devices, our data is being collected and used or sold to others.

-   More importantly, decisions are made by law enforcement, financial institutions, and governments based on data that directly affect the lives of people.

## Privacy of your data {.smaller}

::: question
What pieces of data have you left on the internet today?
Think through everything you've logged into, clicked on, checked in, either actively or automatically, that might be tracking you.
Do you know where that data is stored?
Who it can be accessed by?
Whether it's shared with others?
:::

## Sharing your data {.smaller}

::: question
What are you OK with sharing?
:::

::: columns
::: {.column width="50%"}
::: incremental
-   Name
-   Age
-   Email
-   Phone Number
-   List of every video you watch
-   List of every video you comment on
:::
:::

::: {.column width="50%"}
::: incremental
-   How you type: speed, accuracy
-   How long you spend on different content
-   List of all your private messages (date, time, person sent to)
-   Info about your photos (how it was taken, where it was taken (GPS), when it was taken)
:::
:::
:::

## What does Google think/know about you?

::: question
Have you ever thought about why you're seeing an ad on Google?
Google it!
Try to figure out if you have ad personalization on and how your ads are personalized.
:::

## Your browsing history

::: question
Which of the following are you OK with your browsing history to be used towards?
:::

::: incremental
-   To predict your age for serving you targeted ads
-   To predict your race/ethnicity for serving you targeted ads
-   To score you as a candidate for a job
:::

## Who else gets to use your data?

::: question
Suppose you create a profile on a social media site and share your personal information on your profile.
Who else gets to use that data?
:::

::: incremental
-   Companies the social media company has a connection to?
-   Companies the social media company sells your data to?
-   Researchers?
:::

## OK Cupid data breach

- In 2016, researchers published data of 70,000 OkCupid users‚Äîincluding usernames, political leanings, drug usage, and intimate sexual details

- Researchers didn't release the real names and pictures of OKCupid users, but their identities could easily be uncovered from the details provided, e.g. usernames

## OK Cupid data breach

![](images/14/okcupid-tweet.png)

## OK Cupid data breach

>Some may object to the ethics of gathering and releasing this data. However, all the data found in the dataset are or were already publicly available, so releasing this dataset merely presents it in a more useful form. 
>
>Researchers Emil Kirkegaard and Julius Daugbjerg Bjerrek√¶r

# Misrepresentation

## Misrepresenting data science results {.smaller}

Some common ways people do this, either intentionally or unintentionally, include:

::: incremental
-   Claiming **causality** where it's not in the scope of inference of the underlying study

-   Distorting **axes and scales** to make the data tell a different story

-   Visualizing **spatial areas instead of human density** for issues that depend on and affect humans

-   Omitting **uncertainty** in reporting
:::

## Causality - TIME coverage

::: question
How plausible is the statement in the title of this article?
:::

![](images/14/exercise-cancer-time.png)

::: aside
Alice Park.
[Exercise Can Lower Risk of Some Cancers By 20%](https://time.com/4330041/reduce-cancer-risk-exercise/).
Time Magazine.
16 May 2016.
:::

## Causality - LA Times coverage

::: question
What does "research shows" mean?
:::

![](images/14/exercise-cancer-latimes.png)

::: aside
Melissa Healy.
[Exercising drives down risk for 13 cancers, research shows](https://www.latimes.com/science/sciencenow/la-sci-sn-exercising-cancer-20160516-story.html).\
Los Angeles Times.
16 May 2016.
:::

## Causality - Original study {.smaller}

Moore, Steven C., et al. [**"Association of leisure-time physical activity with risk of 26 types of cancer in 1.44 million adults."**](https://jamanetwork.com/journals/jamainternalmedicine/fullarticle/2521826) JAMA internal medicine 176.6 (2016): 816-825.

-   **Volunteers** were **asked** about their physical activity level over the preceding year.
-   Half exercised less than about 150 minutes per week, half exercised more.
-   Compared to the bottom 10% of exercisers, the top 10% had lower rates of esophageal, liver, lung, endometrial, colon, and breast cancer.
-   Researchers found no association between exercising and 13 other cancers (e.g. pancreatic, ovarian, and brain).

::: notes
Carl Bergstrom and Jevin West.
[Calling Bullshit: The art of skepticism in a data-driven world](https://www.callingbullshit.org/).\
Random House, 2020.\
Sharon Begley.
["Does exercise prevent cancer?"](https://www.statnews.com/2016/05/16/exercise-prevent-cancer/).
StatNews.
16 May 2016.
:::

## Axes and scales - Tax cuts {.smaller}

::: question
What is the difference between these two pictures?
Which presents a better way to represent these data?
:::

![](images/14/axis-start-at-0.png)

::: aside
Christopher Ingraham.
["You‚Äôve been reading charts wrong. Here‚Äôs how a pro does it."](https://www.washingtonpost.com/business/2019/10/14/youve-been-reading-charts-wrong-heres-how-pro-does-it/).
The Washington Post.
14 October 2019.
:::

## Axes and scales - Cost of gas {.smaller}

::: question
What is wrong with this picture?
How would you correct it?
:::

![](images/14/cost-of-gas.png)

## Axes and scales - Cost of gas {.scrollable}

```{r}
#| out-width: 100%
#| echo: false
#| message: false
#| fig-asp: 0.5
library(scales)

df <- tibble(
  date = ymd(c("2019-11-01", "2020-10-25", "2020-11-01")),
  cost = c(3.17, 3.51, 3.57)
)
ggplot(df, aes(x = date, y = cost, group = 1)) +
  geom_point() +
  geom_line() +
  geom_label(aes(label = cost), hjust = -0.25) +
  labs(
    title = "Cost of gas",
    subtitle = "National average",
    x = NULL,
    y = NULL,
    caption = "Source: AAA Fuel Gauge Report"
  ) +
  scale_x_continuous(
    breaks = ymd(c("2019-11-01", "2020-10-25", "2020-11-01")),
    labels = c("Last year", "Last week", "Current"),
    guide = guide_axis(angle = 90),
    limits = ymd(c("2019-11-01", "2020-11-29")),
    minor_breaks = ymd(c("2019-11-01", "2020-10-25", "2020-11-01"))
  ) +
  scale_y_continuous(labels = label_dollar())
```

## Axes and scales - PP services {.smaller}

::: columns
::: {.column width="40%"}
::: question
What is wrong with this picture?
How would you correct it?
:::

:::
::: {.column width="60%"}
![](images/14/pp-misleading.png)
:::
:::

::: aside
Timothy B. Lee.
[Whatever you think of Planned Parenthood, this is a terrible and dishonest chart](https://www.vox.com/2015/9/29/9417845/planned-parenthood-terrible-chart).
Vox.
29 September 2019.
:::

## Axes and scales - PP services {.scrollable}

```{r}
#| echo: false
#| fig-width: 8
#| out-width: 100%
#| fig-asp: 0.4
pp <- tibble(
  year = c(2006, 2006, 2013, 2013),
  service = c("Abortion", "Cancer", "Abortion", "Cancer"),
  n = c(289750, 2007371, 327000, 935573)
)

ggplot(pp, aes(x = year, y = n, color = service)) +
  geom_point(size = 2) +
  geom_line(linewidth = 1) +
  geom_text(aes(label = n), nudge_y = 100000) +
  geom_text(
    aes(label = year), 
    nudge_y = 200000, 
    color = "darkgray"
  ) +
  labs(
    title = "Services provided by Planned Parenthood",
    caption = "Source: Planned Parenthood",
    x = NULL,
    y = NULL
  ) +
  scale_x_continuous(breaks = c(2006, 2013)) +
  scale_y_continuous(labels = label_number(big.mark = ",")) +
  scale_color_manual(values = c("red", "purple")) +
  annotate(
    geom = "text",
    label = "Abortions",
    x = 2009.5,
    y = 400000,
    color = "red"
  ) +
  annotate(
    geom = "text",
    label = "Cancer screening\nand prevention services",
    x = 2010.5,
    y = 1600000, 
    color = "purple"
  ) +
  theme_minimal() +
  theme(legend.position = "none")
```

## Maps and areas - Voting map

::: question
Do you recognize this map?
What does it show?
:::

![](images/14/election-2016-county.png)

::: aside
Lazaro Gamio.
["Election maps are telling you big lies about small things"](https://www.washingtonpost.com/graphics/politics/2016-election/how-election-maps-lie/).
The Washington Post.
1 Nov 2016.
:::

## Maps and areas - Two alternate tales

::::: columns
::: column
![](images/14/citizens-for-trump.png){width="350"}
:::

::: column
![](images/14/counties-for-trump.png){width="350"}
:::
:::::

::: aside
Alberto Cairo.
[Visual Trumpery talk](https://visualtrumperytour.wordpress.com/).
:::

## Maps and areas - Voting percentages

![](images/14/cairo-vote-percentages.png){width="800"}

::: aside
Alberto Cairo.
[Visual Trumpery talk](https://visualtrumperytour.wordpress.com/).
:::

## Maps and areas - Voting percentages

![](images/14/cairo-what-matters.png)

::: aside
Alberto Cairo.
[Visual Trumpery talk](https://visualtrumperytour.wordpress.com/).
:::

## Uncertainty - Catalan independence {.smaller}

On December 19, 2014, the front page of Spanish national newspaper El Pa√≠s read *"Catalan public opinion swings toward 'no' for independence, says survey"*.

```{r}
#| label: catalan-misleading
#| echo: false
#| fig-asp: 0.4
#| out-width: 100%
catalan <- tibble(
  response = c("No", "Yes", "No answer"),
  rate     = c(45.3, 44.5, 10.2)
) |>
  mutate(response = fct_relevel(response, "No", "Yes", "No answer"))

ggplot(catalan, aes(y = fct_rev(response), x = rate, color = response, group = response)) +
  geom_point(size = 3) +
  geom_segment(
    aes(
      x = 0, xend = rate,
      y = fct_rev(response), yend = fct_rev(response)
    ),
    linewidth = 2
  ) +
  scale_color_manual(values = c("#5C8AA9", "#9D303A", "gray")) +
  scale_x_continuous(labels = label_percent(scale = 1)) +
  guides(color = "none") +
  theme_minimal(base_size = 16) +
  labs(
    title = "Do you want Catalonia to become an independent state?",
    caption = "Margin of error: +/-2.95% at 95% confidence level",
    x = NULL, y = NULL
  )
```

::: aside
Alberto Cairo.
[The truthful art: Data, charts, and maps for communication](http://www.thefunctionalart.com/p/the-truthful-art-book.html).
New Riders, 2016.
:::

## Uncertainty - Catalan independence {.smaller}

```{r}
#| label: catalan-corrected
#| echo: false
#| out-width: 100%
#| fig-asp: 0.4
catalan <- catalan |>
  mutate(
    low = rate - 2.95,
    high = rate + 2.95
  )
ggplot(catalan, aes(y = fct_rev(response), x = rate, color = response, group = response)) +
  geom_segment(
    aes(
      x = low, xend = high,
      y = fct_rev(response), 
      yend = fct_rev(response)
    ),
    linewidth = 0.8, color = "black"
  ) +
  geom_point(size = 4) +
  scale_color_manual(values = c("#5C8AA9", "#9D303A", "gray")) +
  scale_x_continuous(labels = label_percent(scale = 1)) +
  guides(color = "none") +
  theme_minimal(base_size = 16) +
  labs(
    title = "Do you want Catalonia to become an independent state?",
    caption = "The probability of the tiny difference between the\n'No' and 'Yes' being just due to random chance is very high.",
    x = NULL, y = NULL
  )
```

::: aside
Alberto Cairo.
["Uncertainty and Graphicacy: How Should Statisticians Journalists and Designers Reveal Uncertainty in Graphics for Public Consumption?"](https://ec.europa.eu/eurostat/cros/powerfromstatistics/OR/PfS-OutlookReport-Cairo.pdf), Power from Statistics: Data Information and Knowledge, 2017.
:::

# Algorithmic bias

## Garbage in, garbage out

- In statistical modeling and inference we talk about "garbage in, garbage out" ‚Äì if you don‚Äôt have good (random, representative) data, results of your analysis will not be reliable or generalizable.

- Corollary: Bias in, bias out.

## Google Translate {.smaller}

::: question
What might be the reason for Google‚Äôs gendered translation? How do ethics play into this situation?
:::

![](images/14/google-translate.png)

::: aside
Source: [Engadget - Google is working to remove gender bias in its translations](https://www.engadget.com/2018-12-07-google-remove-gender-bias-translations.html)
:::

## Criminal sentencing

2016 ProPublica article on algorithm used for rating a defendant's risk of future crime:

![](images/14/machine-bias-cover.png){width=500}

::: aside
Source: Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner.
[Machine Bias](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing).
23 May 2016.
ProPublica.
:::

## Risk score errors

::: question
What is common among the defendants who were assigned a high/low risk score for reoffending?
:::

::::: columns
::: {.column width="35%"}
![](images/14/machine-bias-petty-theft-1.png){fig-align="center" width="400" height="333"} ![](images/14/machine-bias-petty-theft-2.png){fig-align="center" width="400" height="240"}
:::

::: {.column width="35%"}
![](images/14/machine-bias-drug-posession-1.png){fig-align="center" width="400" height="333"} ![](images/14/machine-bias-drug-posession-2.png){fig-align="center" width="400" height="240"}
:::
:::::

## ProPublica analysis {.smaller}

**Data:** Risk scores assigned to \>7,000 people arrested in Broward County, FL + whether they were charged with new crimes over the following 2 years.

. . .

**Results:**

::: incremental
-   20% of those predicted to commit violent crimes actually did.
-   Algorithm had higher accuracy (61%) when full range of crimes taken into account (e.g. misdemeanors). ![](images/14/propublica-results.png)
-   Algorithm was more likely to falsely flag black defendants as future criminals, at almost twice the rate as white defendants.
-   White defendants were mislabeled as low risk more often than black defendants.
:::

## Risk scores

::: question
How can an algorithm that doesn't use race as input data be racist?
:::

![](images/14/machine-bias-risk-scores.png){fig-align="center" width="600"}

## Predicting ethnicity {.xxsmall}

[Improving Ecological Inference by Predicting Individual Ethnicity from Voter Registration Record](https://imai.fas.harvard.edu/research/race.html) (Imran and Khan, 2016)

> In both political behavior research and voting rights litigation, turnout and vote choice for different racial groups are often inferred using aggregate election results and racial composition.
> Over the past several decades, many statistical methods have been proposed to address this ecological inference problem.
> We propose an alternative method to reduce aggregation bias by predicting individual-level ethnicity from voter registration records.
> Building on the existing methodological literature, we use Bayes‚Äôs rule to combine the Census Bureau‚Äôs Surname List with various information from geocoded voter registration records.
> We evaluate the performance of the proposed methodology using approximately nine million voter registration records from Florida, where self-reported ethnicity is available.
> We find that it is possible to reduce the false positive rate among Black and Latino voters to 6% and 3%, respectively, while maintaining the true positive rate above 80%.
> Moreover, we use our predictions to estimate turnout by race and find that our estimates yields substantially less amounts of bias and root mean squared error than standard ecological inference estimates.
> We provide open-source software to implement the proposed methodology.
> The open-source software is available for implementing the proposed methodology.

## **wru** package

The said ‚Äúsource software‚Äù is the wru package: <https://github.com/kosukeimai/wru>.

::: question
Do you have any ethical concerns about installing this package?
:::

## **wru** package {.smaller}

::: question
Was the publication of this model ethical?
Does the open-source nature of the code affect your answer?
Is it ethical to use this software?
Does your answer change depending on the intended use?
:::

```{r}
#| cache: true
#| message: false
library(wru)
predict_race(voter.file = voters, surname.only = TRUE) |>
  select(surname, contains("pred"))
```

## **wru** package {.smaller}

```{r}
#| cache: true
#| message: false
#| warning: false

me <- tibble(surname = "√áetinkaya-Rundel")

predict_race(voter.file = me, surname.only = TRUE)
```

# Further reading

## How Charts Lie

::::: columns
::: column
![](images/14/cairo-how-charts-lie.jpg){width="400"}
:::

::: column
[How Charts Lie](http://www.thefunctionalart.com/p/reviews.html)

Getting Smarter about Visual Information

by Alberto Cairo
:::
:::::

## Calling Bullshit

::::: columns
::: column
![](images/14/calling-bullshit.png){width="400"}
:::

::: column
[Calling Bullshit](https://www.callingbullshit.org/)\
The Art of Skepticism in a\
Data-Driven World

by Carl Bergstrom and Jevin West
:::
:::::

## Machine Bias

::::: columns
::: column
![](images/14/machine-bias-cover.png){width="400"}
:::

::: column
[Machine Bias](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing) \

by Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner
:::
:::::

## Ethics and Data Science

::::: columns
::: column
![](images/14/ethics-data-science.jpg){width="400"}
:::

::: column
[Ethics and Data Science](https://www.amazon.com/Ethics-Data-Science-Mike-Loukides-ebook/dp/B07GTC8ZN7) \

by Mike Loukides, Hilary Mason, DJ Patil  
(Free Kindle download)
:::
:::::

## Weapons of Math Destruction

::::: columns
::: column
![](images/14/weapons-of-math-destruction.jpg){width="400"}
:::

::: column
[Weapons of Math Destruction](https://www.penguin.co.uk/books/304/304514/weapons-of-math-destruction/9780141985411.html)  
How Big Data Increases Inequality and Threatens Democracy \

by Cathy O'Neil
:::
:::::

## Algorithms of Oppression

::::: columns
::: column
![](images/14/algorithms-of-oppression.jpg){width="400"}
:::

::: column
[Algorithms of Oppression](https://nyupress.org/9781479837243/algorithms-of-oppression/)  
How Search Engines Reinforce Racism \

by Safiya Umoja Noble
:::
:::::

## And more recently...

[How AI discriminates and what that means for your Google habit](https://newsroom.ucla.edu/stories/how-ai-discriminates-and-what-that-means-for-your-google-habit)  
A conversation with UCLA internet studies scholar Safiya Noble \

by Julia Busiek

## Parting thoughts {.smaller}

- At some point during your data science learning journey you will learn tools that can be used unethically

- You might also be tempted to use your knowledge in a way that is ethically questionable either because of business goals or for the pursuit of further knowledge (or because your boss told you to do so)

::: question
How do you train yourself to make the right decisions (or reduce the likelihood of accidentally making the wrong decisions) at those points?
:::
