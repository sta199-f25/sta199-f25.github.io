---
title: "Language of models"
subtitle: "Lecture 15"
date: "2025-10-21"
format: 
  revealjs: 
    output-file: 15-language-of-models-slides.html
    footer: "[🔗 sta199-f25.github.io](https://sta199-f25.github.io/)"
    theme: slides.scss
    transition: fade
    slide-number: true
    logo: images/logo.png
    pdf-separate-fragments: true
    toc: false
  html: 
    code-link: true
filters: 
  - ../remove-fmt-skip.lua
editor_options: 
  chunk_output_type: console
---

```{r}
#| label: setup
#| include: false
library(tidyverse)
library(gt)
ggplot2::theme_set(ggplot2::theme_gray(base_size = 24))
todays_ae <- "ae-10-modeling-fish"
```

# Warm-up

## While you wait: Participate 📱💻 {.xsmall}

:::::: columns
:::: {.column width="70%"}
::: wooclap
What might have been the reason for Google’s gendered translation?
How do ethics play into this situation?

![](images/15/google-translate.png)

:::
::::

::: {.column width="30%"}
{{< include _wooclap-column.qmd >}}
:::
::::::

::: aside
Source: [Engadget - Google is working to remove gender bias in its translations](https://www.engadget.com/2018-12-07-google-remove-gender-bias-translations.html)
:::

## Announcements {.smaller}

-   Peer evaluation 2 due on Thursday at 11:59 pm

-   You must continue to make progress on your projects until Sunday evening in preparation for peer evaluations on Monday in lab

# From last time: Algorithmic bias

## Garbage in, garbage out

-   In statistical modeling and inference we talk about "garbage in, garbage out" – if you don’t have good (random, representative) data, results of your analysis will not be reliable or generalizable.

-   Corollary: Bias in, bias out.

## Criminal sentencing

2016 ProPublica article on algorithm used for rating a defendant's risk of future crime:

![](images/15/machine-bias-cover.png){width="500"}

::: aside
Source: Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner.
[Machine Bias](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing).
23 May 2016.
ProPublica.
:::

## Risk score errors

::: question
What is common among the defendants who were assigned a high/low risk score for reoffending?
:::

::::: columns
::: {.column width="35%"}
![](images/15/machine-bias-petty-theft-1.png){fig-align="center" width="400" height="300"} ![](images/15/machine-bias-petty-theft-2.png){fig-align="center" width="400" height="200"}
:::

::: {.column width="35%"}
![](images/15/machine-bias-drug-posession-1.png){fig-align="center" width="400" height="300"} ![](images/15/machine-bias-drug-posession-2.png){fig-align="center" width="400" height="200"}
:::
:::::

## ProPublica analysis {.smaller}

**Data:** Risk scores assigned to \>7,000 people arrested in Broward County, FL + whether they were charged with new crimes over the following 2 years.

**Results:**

::: incremental
-   20% of those predicted to commit violent crimes actually did.
-   Algorithm had higher accuracy (61%) when full range of crimes taken into account (e.g. misdemeanors). ![](images/15/propublica-results.png)
-   Algorithm was more likely to falsely flag black defendants as future criminals, at almost twice the rate as white defendants.
-   White defendants were mislabeled as low risk more often than black defendants.
:::

## Risk scores

::::: columns
::: {.column width="35%"}

::: question
How can an algorithm that doesn't use race as input data be racist?
:::

:::

::: {.column width="65%"}

![](images/15/machine-bias-risk-scores.png){fig-align="center" width="600"}

:::

:::

## Predicting ethnicity {.xxsmall}

[Improving Ecological Inference by Predicting Individual Ethnicity from Voter Registration Record](https://imai.fas.harvard.edu/research/race.html) (Imran and Khan, 2016)

> In both political behavior research and voting rights litigation, turnout and vote choice for different racial groups are often inferred using aggregate election results and racial composition.
> Over the past several decades, many statistical methods have been proposed to address this ecological inference problem.
> We propose an alternative method to reduce aggregation bias by predicting individual-level ethnicity from voter registration records.
> Building on the existing methodological literature, we use Bayes’s rule to combine the Census Bureau’s Surname List with various information from geocoded voter registration records.
> We evaluate the performance of the proposed methodology using approximately nine million voter registration records from Florida, where self-reported ethnicity is available.
> We find that it is possible to reduce the false positive rate among Black and Latino voters to 6% and 3%, respectively, while maintaining the true positive rate above 80%.
> Moreover, we use our predictions to estimate turnout by race and find that our estimates yields substantially less amounts of bias and root mean squared error than standard ecological inference estimates.
> We provide open-source software to implement the proposed methodology.
> The open-source software is available for implementing the proposed methodology.

## **wru** package

The said “source software” is the wru package: <https://github.com/kosukeimai/wru>.

::: question
Do you have any ethical concerns about installing this package?
:::

## **wru** package {.smaller}

::: question
Was the publication of this model ethical?
Does the open-source nature of the code affect your answer?
Is it ethical to use this software?
Does your answer change depending on the intended use?
:::

```{r}
#| eval: false
library(wru)
predict_race(voter.file = voters, surname.only = TRUE) |>
  select(surname, contains("pred"))
```

```{r}
#| cache: true
#| message: false
#| echo: false
options(width = 100)
library(wru)
predict_race(voter.file = voters, surname.only = TRUE) |>
  select(surname, contains("pred"))
```

## **wru** package {.smaller}

```{r}
#| cache: true
#| message: false
#| warning: false
me <- tibble(surname = "Çetinkaya-Rundel")
predict_race(voter.file = me, surname.only = TRUE)
```

## Parting thoughts {.smaller}

-   At some point during your data science learning journey you will learn tools that can be used unethically

-   You might also be tempted to use your knowledge in a way that is ethically questionable either because of business goals or for the pursuit of further knowledge (or because your boss told you to do so)

::: question
How do you train yourself to make the right decisions (or reduce the likelihood of accidentally making the wrong decisions) at those points?
:::

# Modeling

## Two main 

- Prediction / classification

- Description / explanation

<br>

::: question

Can you think of examples of modeling for prediction vs. modeling for explanation?

:::

## Prediction gone wrong...

> Tesla thinks my garage is a semi...

![](images/15/tesla-get-wrong-1.png){fig-align="center" width="300"}

::: aside
Source: [Reddit](https://www.reddit.com/r/TeslaModelY/comments/112520t/new_owner_here_just_parked_in_my_garage_tesla/)
:::

## Leisure, commute, physical activity & BP {.smaller .scrollable}

[Relation Between Leisure Time, Commuting, and Occupational Physical Activity With Blood Pressure in 125,402 Adults: The Lifelines Cohort](https://www.ahajournals.org/doi/full/10.1161/JAHA.119.014313)

**Background:** Whether all domains of daily‐life moderate‐to‐vigorous physical activity (MVPA) are associated with lower blood pressure (BP) and how this association depends on age and body mass index remains unclear.

**Methods and Results:** In the population‐based Lifelines cohort (N=125,402), MVPA was assessed by the Short Questionnaire to Assess Health‐Enhancing Physical Activity, a validated questionnaire in different domains such as commuting, leisure‐time, and occupational PA.
BP was assessed using the last 3 of 10 measurements after 10 minutes’ rest in the supine position.
Hypertension was defined as systolic BP ≥140 mm Hg and/or diastolic BP ≥90 mm Hg and/or use of antihypertensives.
In regression analysis, higher commuting and leisure‐time but not occupational MVPA related to lower BP and lower hypertension risk.
Commuting‐and‐leisure‐time MVPA was associated with BP in a dose‐dependent manner.
β Coefficients (95% CI) from linear regression analyses were −1.64 (−2.03 to −1.24), −2.29 (−2.68 to −1.90), and finally −2.90 (−3.29 to −2.50) mm Hg systolic BP for the low, middle, and highest tertile of MVPA compared with “No MVPA” as the reference group after adjusting for age, sex, education, smoking and alcohol use.
Further adjustment for body mass index attenuated the associations by 30% to 50%, but more MVPA remained significantly associated with lower BP and lower risk of hypertension.
This association was age dependent.
β Coefficients (95% CI) for the highest tertiles of commuting‐and‐leisure‐time MVPA were −1.67 (−2.20 to −1.15), −3.39 (−3.94 to −2.82) and −4.64 (−6.15 to −3.14) mm Hg systolic BP in adults \<40, 40 to 60, and \>60 years, respectively.

**Conclusions:** Higher commuting and leisure‐time but not occupational MVPA were significantly associated with lower BP and lower hypertension risk at all ages, but these associations were stronger in older adults.

::: aside

Byambasukh, Oyuntugs, Harold Snieder, and Eva Corpeleijn.
> "Relation between leisure time, commuting, and occupational physical activity with blood pressure in 125 402 adults: the lifelines cohort." *Journal of the American Heart Association* 9.4 (2020): e014313.

:::

# Modeling

## Modeling cars {.smaller}

::: question
-   What is the relationship between cars' weights and their mileage?
-   What is your best guess for a car's MPG that weighs 3,500 pounds?
:::

```{r}
#| echo: false
base <- ggplot(mtcars, aes(x = wt, y = mpg)) +
  geom_point() +
  labs(
    x = "Weight (1,000 lbs)",
    y = "Miles per gallon (MPG)",
    title = "MPG vs. weights of cars"
  ) +
  coord_cartesian(xlim = c(1.5, 5.5), ylim = c(10, 35))

base
```

## Modelling cars {.smaller}

::: question
**Describe:** What is the relationship between cars' weights and their mileage?
:::

```{r}
#| echo: false
#| message: false
base +
  geom_smooth(method = "lm", color = "#BE394F")
```

## Modelling cars {.smaller}

::: question
**Predict:** What is your best guess for a car's MPG that weighs 3,500 pounds?
:::

```{r}
#| echo: false
#| message: false
base +
  geom_smooth(
    method = "lm",
    se = FALSE,
    color = "darkgray",
    linetype = "dashed"
  ) +
  annotate(
    "segment",
    x = 3.5,
    xend = 3.5,
    y = -Inf,
    yend = 18.5,
    color = "#BE394F"
  ) +
  annotate(
    "segment",
    x = -Inf,
    xend = 3.5,
    y = 18.5,
    yend = 18.5,
    color = "#BE394F"
  )
```

## Modelling

-   Use models to explain the relationship between variables and to make predictions
-   For now we will focus on **linear** models (but there are *many* *many* other types of models too!)

## Modelling vocabulary

-   Predictor (explanatory variable)
-   Outcome (response variable)
-   Regression line
    -   Slope
    -   Intercept
-   Correlation

## Predictor (explanatory variable)

```{r}
#| echo: false
base <- ggplot(mtcars, aes(x = wt, y = mpg)) +
  geom_point() +
  labs(
    x = "Weight (1000 lbs)",
    y = "Miles per gallon (MPG)",
    title = "MPG vs. weights of cars"
  )
```

:::::: columns
::: {.column width="25%"}
```{r}
#| echo: false
mtcars |>
  select(mpg, wt) |>
  slice_head(n = 6) |>
  mutate(across(where(is.numeric), as.character)) |>
  bind_rows(c(mpg = "...", wt = "...")) |>
  gt() |>
  tab_style(
    style = list(
      cell_fill(color = "#BE394F"),
      cell_text(color = "white")
      ),
    locations = cells_body(columns = wt)
  ) |>
  tab_options(table.font.size = px(24))
```
:::

::: {.column width="5%"}
:::

::: {.column width="70%"}
```{r}
#| echo: false
base +
  theme(
    axis.title.x = element_text(color = "#BE394F", face = "bold", size = 16)
  )
```
:::
::::::

## Outcome (response variable)

:::::: columns
::: {.column width="25%"}
```{r}
#| echo: false
mtcars |>
  select(mpg, wt) |>
  slice_head(n = 6) |>
  mutate(across(where(is.numeric), as.character)) |>
  bind_rows(c(mpg = "...", wt = "...")) |>
  gt() |>
  tab_style(
    style = list(
      cell_fill(color = "#BE394F"),
      cell_text(color = "white")
      ),
    locations = cells_body(columns = mpg)
  ) |>
  tab_options(table.font.size = px(24))
```
:::

::: {.column width="5%"}
:::

::: {.column width="70%"}
```{r}
#| echo: false
base +
  theme(
    axis.title.y = element_text(color = "#BE394F", face = "bold", size = 16)
  )
```
:::
::::::

## Regression line

```{r}
#| echo: false
#| message: false
base +
  geom_smooth(method = "lm", color = "#BE394F", linewidth = 1.5, se = FALSE)
```

## Regression line: slope

```{r}
#| echo: false
#| message: false
base +
  geom_smooth(method = "lm", color = "black", se = FALSE) +
  annotate(
    geom = "segment",
    x = 4, xend = 5, y = 16, yend = 16, 
    linetype = "dashed", color = "#BE394F"
  ) +
  annotate(
    geom = "segment",
    x = 5, xend = 5, y = 16, yend = 10.6, 
    color = "#BE394F"
  ) +
  annotate(
    geom = "text",
    x = 5.2, y = 13, label = "slope", 
    color = "#BE394F", size = 5, hjust = 0
  )
```

## Regression line: intercept

```{r}
#| echo: false
#| message: false
base +
  geom_smooth(method = "lm", color = "gray", se = FALSE, fullrange = TRUE, linetype = "dashed") +
  geom_smooth(method = "lm", color = "black", se = FALSE) +
  scale_x_continuous(limits = c(0, 5.5)) +
  annotate(
    geom = "point",
    shape = 1, size = 4, stroke = 2,
    x = 0, y = 37.4, 
    color = "#BE394F"
  ) +
  annotate(
    geom = "text",
    label = "intercept",
    x = 0.5, y = 37.4, 
    color = "#BE394F", size = 5, hjust = 0
  )
```

## Correlation

```{r}
#| echo: false
r <- mtcars |>
  summarize(cor = round(cor(mpg, wt), 2)) |>
  pull()

base +
  stat_ellipse(geom = "polygon", color = "#BE394F", fill = "#BE394F30") +
  annotate(
    geom = "text",
    x = 3.5, y = 27.5, 
    label = paste("r =", r),
    color = "#BE394F", size = 5, hjust = 0
  )
```

## Correlation

-   Ranges between -1 and 1.
-   Same sign as the slope.

```{r}
#| fig-asp: 0.2
#| echo: false
set.seed(12345)
df <- tibble(
  x = 1:20,
  y1 = 1:20 * 0.75,
  y2 = 1:20 * 0.75 + rnorm(20, 0, 1),
  y3 = 1:20 * 0.75 + rnorm(20, 0, 8),
  y4 = 1:20 + rnorm(20, 0, 200),
  y5 = -1 * y3,
  y6 = -1 * y2,
  y7 = -1 * y1
) |>
  pivot_longer(cols = starts_with("y"), names_to = "series", values_to = "y") |>
  group_by(series) |>
  mutate(r = paste("r =", round(cor(x, y), 2))) |>
  ungroup() |>
  arrange(series, x) |>
  mutate(r = fct_inorder(r))

ggplot(df, aes(x = x, y = y, color = r)) +
  geom_point(show.legend = FALSE) +
  facet_wrap(~r, ncol = 7, scales = "free") +
  theme_bw(base_size = 16) +
  labs(x = NULL, y = NULL) +
  theme(
    axis.text = element_blank(),
    axis.ticks = element_blank()
  ) +
  scale_color_viridis_d(end = 0.9)
```

## Participate 📱💻 {.xsmall}

:::::: columns
:::: {.column width="70%"}
::: wooclap

Which of the following is the best guess for the correlation between the to variables on the plot below?

```{r}
#| echo: false
#| fig-width: 3
#| fig-asp: 1
set.seed(54321)
df <- tibble(
  x = 1:20,
  y = 1:20 + rnorm(20, 0, 200)
)

ggplot(df, aes(x = x, y = y)) +
  geom_point() +
  labs(x = NULL, y = NULL) +
  theme(
    axis.text = element_blank(),
    axis.ticks = element_blank()
  )
```

::: wooclap-choices

- -0.95

- -0.53

- 0.00

- 0.25

- 0.80
:::

:::
::::

::: {.column width="30%"}
{{< include _wooclap-column.qmd >}}
:::
::::::


## Visualizing the model

```{r}
ggplot(mtcars, aes(x = wt, y = mpg)) +
  geom_point() +
  geom_smooth(method = "lm")
```

# Application exercise

## `{r} todays_ae` {.smaller}

::: appex
-   Go to your ae project in RStudio.

-   If you haven't yet done so, make sure all of your changes up to this point are committed and pushed, i.e., there's nothing left in your Git pane.

-   If you haven't yet done so, click Pull to get today's application exercise file: *`{r} paste0(todays_ae, ".qmd")`*.

-   Work through the application exercise in class, and render, commit, and push your edits.
:::