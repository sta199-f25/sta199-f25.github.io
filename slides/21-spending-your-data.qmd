---
title: "Spending your data"
subtitle: "Lecture 21"
date: "2025-11-11"
format: 
  revealjs: 
    output-file: 21-spending-your-data-slides.html
    footer: "[ðŸ”— sta199-f25.github.io](https://sta199-f25.github.io/)"
    theme: slides.scss
    transition: fade
    slide-number: true
    logo: images/logo.png
    pdf-separate-fragments: true
    toc: false
  html: 
    code-link: true
filters: 
  - ../remove-fmt-skip.lua
editor_options: 
  chunk_output_type: console
---

```{r}
#| label: setup
#| include: false
ggplot2::theme_set(ggplot2::theme_bw(base_size = 24))
todays_ae <- "ae-14-chicago-taxi-classification"
```

# Warm-up

## While you wait: Participate ðŸ“±ðŸ’» {.xsmall}

::: columns

::: {.column width="70%"}

::: wooclap

Which of the following files should you be updating in your project between now and Thursday's deadline? Check all that apply.

::: wooclap-choices
- `contract.qmd`
- `index.qmd`
- `presentation.qmd` or `presentation.pdf`
- `proposal.qmd`
- `README.md` in your `data` folder
:::

:::

:::

::: {.column width="30%"}
{{< include _wooclap-column.qmd >}}
:::

:::

## Announcements {.smaller}

-   Project write-ups due Thursday at 11:59pm

-   Peer evaluations due Friday at 11:59pm

## From last week

::: appex
Finish up `ae-13-spam-filter`.
:::

# Setup

## Packages

```{r}
#| label: load-packages
#| message: false
library(tidyverse) # data wrangling and visualization
library(tidymodels) # modeling
```

## Data

-   Taxi trips in Chicago, IL in 2022

-   Data from [Chicago Data Portal](https://data.cityofchicago.org/Transportation/Taxi-Trips/wrvz-psew)

## Data: `chicago_taxi` {.smaller}

```{r}
#| include: false
chicago_taxi <- read_csv(here::here("slides", "data/chicago-taxi.csv"))
```

```{r}
#| eval: false
chicago_taxi <- read_csv("data/chicago-taxi.csv")
```

```{r}
chicago_taxi
```

## Data: `chicago_taxi` {.smaller}

```{r}
glimpse(chicago_taxi)
```

## Data prep

```{r}
chicago_taxi <- chicago_taxi |>
  mutate(
    tip = fct_relevel(tip, "no", "yes"),
    local = fct_relevel(local, "no", "yes"),
    dow = fct_relevel(dow, "Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun"),
    month = fct_relevel(month, "Jan", "Feb", "Mar", "Apr")
  )
```

## Outcome and predictors {.smaller}

- Outcome - `tip`: Whether rider left a tip -- a factor w/ levels "yes" and "no"
- (Potential) predictors:
  - Numerical:
    - `distance`: The trip distance, in odometer miles.
    - `hour`: The hour of the day in which the trip began.
  - Categorical:
    - `company`: The taxi company -- companies that occurred few times were binned as "other".
    - `local`: Whether the trip started in the same community area as it began.
    - `dow`: The day of the week in which the trip began.
    - `month`: The month in which the trip began.

## Should we include a predictor?

To determine whether we should include a predictor in a model, we should start by asking:

::: incremental
-   Is it ethical to use this variable?
    (Or even legal?)

-   Will this variable be available at prediction time?

-   Does this variable contribute to explainability?
:::

# Data splitting and spending

## We've been cheating!

::: incremental
-   So far, we've been using all the data we have for building models.
    In predictive contexts, this would be considered *cheating*.

-   Evaluating model performance for predicting outcomes that were used when building the models is like evaluating your learning with questions whose answers you've already seen.
:::

## Spending your data {.smaller}

For predictive models (used primarily in machine learning), we typically split data into training and test sets:

![](images/21/test-train-split-1.svg){fig-align="center"}

-   The **training set** is used to estimate model parameters.

-   The **test set** is used to find an independent assessment of model performance.

. . .

::: callout-warning
Do not use, or even peek at, the test set during training.
:::

## How much to spend?

::: incremental
-   The more data we spend (use in training), the better estimates weâ€™ll get.

-   Spending too much data in training prevents us from computing a good assessment of predictive performance.

-   Spending too much data in testing prevents us from computing a good estimate of model parameters.
:::

## The initial split

```{r}
#| label: initial-split
set.seed(20251111)
chicago_taxi_split <- initial_split(chicago_taxi)
chicago_taxi_split
```

## Setting a seed {.smaller}

::: task
What does `set.seed()` do?
:::

::: incremental
-   To create that split of the data, R generates â€œpseudo-randomâ€ numbers: while they are made to behave like random numbers, their generation is deterministic given a â€œseedâ€.

-   This allows us to reproduce results by setting that seed.

-   Which seed you pick doesnâ€™t matter, as long as you donâ€™t try a bunch of seeds and pick the one that gives you the best performance.
:::

## Accessing the data

```{r}
#| label: access-data
chicago_taxi_train <- training(chicago_taxi_split)
chicago_taxi_test <- testing(chicago_taxi_split)
```

## The training set {.smaller}

```{r}
chicago_taxi_train
```

## The testing data

. . .

::: huge
ðŸ™ˆ
:::

# Exploratory data analysis

## Initial questions {.smaller}

-   Whatâ€™s the distribution of the outcome, `tip`?

-   Whatâ€™s the distribution of numeric variables like `distance` or `hour`?

-   How does the distribution of `tip` differ across the categorical and numerical variables?

## While you wait: Participate ðŸ“±ðŸ’» {.xsmall}

::: columns

::: {.column width="70%"}

::: wooclap

Which dataset should we use for the exploration?

::: wooclap-choices
- The entire data `chicago_taxi`
- The training data `chicago_taxi_train`
- The testing data `chicago_taxi_test`
:::

:::

:::

::: {.column width="30%"}
{{< include _wooclap-column.qmd >}}
:::

:::

## `tip`

Whatâ€™s the distribution of the outcome, `tip`?

```{r}
chicago_taxi_train |>
  count(tip) |>
  mutate(prop = n / sum(n))
```

## `distance`

Whatâ€™s the distribution of `distance`?

```{r}
ggplot(chicago_taxi_train, aes(x = distance)) +
  geom_histogram(binwidth = 3)
```

## `tip` and `distance` {.smaller}

```{r}
#| code-line-numbers: "|7"
ggplot(chicago_taxi_train, aes(x = distance, fill = tip, group = tip)) +
  geom_histogram(binwidth = 3, show.legend = FALSE) +
  scale_fill_manual(values = c("yes" = "darkolivegreen4", "no" = "darkgray")) +
  facet_wrap(~tip, ncol = 1, labeller = label_both, scales = "free_y")
```

## `tip` and `hour` {.smaller}

```{r}
ggplot(chicago_taxi_train, aes(x = hour, fill = tip, group = tip)) +
  geom_histogram(binwidth = 1, show.legend = FALSE) +
  scale_fill_manual(values = c("yes" = "darkolivegreen4", "no" = "darkgray")) +
  scale_x_continuous(breaks = seq(0, 18, by = 6)) +
  facet_wrap(~tip, ncol = 1, labeller = label_both, scales = "free_y")
```

## `tip` and `local` {.smaller}

```{r}
#| code-line-numbers: "|2"
ggplot(chicago_taxi_train, aes(x = local, fill = tip)) +
  geom_bar(position = "fill", color = "white") +
  scale_fill_manual(values = c("yes" = "darkolivegreen4", "no" = "darkgray")) +
  labs(y = "Proportion") +
  theme(legend.position = "top")
```

## `tip` and `dow` {.smaller}

```{r}
ggplot(chicago_taxi_train, aes(x = dow, fill = tip)) +
  geom_bar(position = "fill", color = "white") +
  scale_fill_manual(values = c("yes" = "darkolivegreen4", "no" = "darkgray")) +
  labs(y = "Proportion", x = "Day of week") +
  theme(legend.position = "top")
```

# Terminology

## False negative and positive

-   **False negative rate** is the proportion of actual positives that were classified as negatives.

-   **False positive rate** is the proportion of actual negatives that were classified as positives.

## Sensitivity

**Sensitivity** is the proportion of actual positives that were correctly classified as positive.

-   Also known as **true positive rate** and **recall**

-   Sensitivity = 1 âˆ’ False negative rate

-   Useful when false negatives are more "expensive" than false positives

## Specificity

**Specificity** is the proportion of actual negatives that were correctly classified as negative

-   Also known as **true negative rate**

-   Specificity = 1 âˆ’ False positive rate

## ROC curve

The **receiver operating characteristic (ROC) curve** allows to assess the model performance across a range of thresholds.

![](images/21/roc-curve.png)

## Participate ðŸ“±ðŸ’» {.xsmall}

::: columns

::: {.column width="70%"}

::: wooclap

Which of the following best describes the area annotated on the ROC curve?

![](images/21/roc-curve-annotated-question.png)

::: wooclap-choices
- Where all positives classified as positive, all negatives classified as negative
- Where true positive rate = false positive rate
- Where all positives classified as negative, all negatives classified as positive
:::

:::

:::

::: {.column width="30%"}
{{< include _wooclap-column.qmd >}}
:::

:::
