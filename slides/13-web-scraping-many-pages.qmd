---
title: "Web scraping<br>many pages"
subtitle: "Lecture 14"
date: "2025-10-09"
format: 
  revealjs: 
    output-file: 13-web-scraping-many-pages-slides.html
    footer: "[ðŸ”— sta199-f25.github.io](https://sta199-f25.github.io/)"
    theme: slides.scss
    transition: fade
    slide-number: true
    logo: images/logo.png
    pdf-separate-fragments: true
    toc: false
  html: 
    code-link: true
filters: 
  - ../remove-fmt-skip.lua
editor_options: 
  chunk_output_type: console
---

```{r}
#| label: setup
#| include: false
ggplot2::theme_set(ggplot2::theme_gray(base_size = 24))
todays_ae <- "ae-09-chronicle-scrape"
```

# Warm-up

## While you wait: Participate ðŸ“±ðŸ’» {.xsmall}

::: columns

::: {.column width="70%"}

::: wooclap

The following code in `chronicle-scrape.R` extracts titles of an opinion article from The Chronicle website:

```{r}
#| eval: false
page <- read_html(
  "https://www2.stat.duke.edu/~cr173/data/dukechronicle-opinion/www.dukechronicle.com/section/opinionabc4.html"
)

titles <- page |>
  html_elements(".space-y-4 .font-extrabold") |>
  html_text()
```

Which of the following needs to change to extract column titles instead?

::: wooclap-choices
- Change the URL in `read_html()`
- Change the function `html_elements()` to `html_element()`
- Change the CSS selector `.space-y-4 .font-extrabold` to `.space-y-4 .text-brand`
- Change the function `html_text()` to `html_attr()`
:::

:::

:::

::: {.column width="30%"}
{{< include _wooclap-column.qmd >}}
:::

:::

## Announcements {.smaller}

- HW 2, Question 7: Reproduce the colorful box plot -- We caught an error in grading (any theme with a white background would have worked). If you originally missed points due to not using `theme_bw()`, but you used another theme with a white background, we've updated your grade.

- Midsemester course survey due tonight at 11:59pm

- Project proposals (Milestone 2) + first peer evaluation due next Thursday at 11:59pm -- any questions?

# From last time

## Opinion articles in The Chronicle {.smaller}

Go to <https://www2.stat.duke.edu/~cr173/data/dukechronicle-opinion/www.dukechronicle.com/section/opinionabc4.html> (copy of The Chronicle opinion section as of October 7, 2025).

## Goal

::::: columns
::: {.column width="50%"}
-   Scrape data and organize it in a tidy format in R
-   Perform light text parsing to clean data
-   Summarize and visualze the data
:::

::: {.column width="50%"}
![](images/13/chronicle-data.png){fig-align="center"}
:::
:::::

## `{r} todays_ae` {.smaller}

::: appex
-   Go to your ae project in RStudio.

-   If you haven't yet done so, make sure all of your changes up to this point are committed and pushed, i.e., there's nothing left in your Git pane.

-   If you haven't yet done so, click Pull to get today's application exercise file: *`{r} paste0(todays_ae, ".qmd")`* and *`chronicle-scrape.R`*.
:::

## Participate ðŸ“±ðŸ’» {.xsmall}

::: columns

::: {.column width="70%"}

::: wooclap

Put the folllowing tasks in order to scrape data from a website:

::: wooclap-choices
- Use the SelectorGadget identify tags for elements you want to grab
- Use `read_html()` to read the page's source code into R
- Use other functions from the **rvest** package to parse the elements you're interested in
- Put the components together in a data frame (a tibble) and analyze it like you analyze any other data
:::

:::

:::

::: {.column width="30%"}
{{< include _wooclap-column.qmd >}}
:::

:::

## A new R workflow {.smaller}

-   When working in a Quarto document, your analysis is re-run each time you knit

-   If web scraping in a Quarto document, you'd be re-scraping the data each time you knit, which is undesirable (and not *nice*)!

-   An alternative workflow:

    -   Use an R script to save your code
    -   Saving interim data scraped using the code in the script as CSV or RDS files
    -   Use the saved data in your analysis in your Quarto document

# Web scraping considerations

## Ethics: "Can you?" vs "Should you?" {.smaller}

![](images/13/ok-cupid-1.png){fig-align="center" width="700"}

::: aside
Source: Brian Resnick, [Researchers just released profile data on 70,000 OkCupid users without permission](https://www.vox.com/2016/5/13/11666116/70000-okcupid-users-data-release), Vox.
:::

## "Can you?" vs "Should you?"

![](images/13/ok-cupid-2.png){fig-align="center" width="699"}

## Challenges: Unreliable formatting

![](images/13/unreliable-formatting.png){fig-align="center" width="699"}

## Challenges: Data broken into many pages

![](images/13/many-pages.png){fig-align="center"}


# Scraping from many pages

## Packages

```{r}
#| message: false
library(tidyverse)
library(rvest) # for web scraping
library(knitr) # for pretty tables
```

## Columns and words

::: question
What additional information do we need to produce the table below?
:::

```{r}
#| include: false
chronicle_article <- read_csv("data/chronicle-article.csv")
```

```{r}
#| label: chronicle-article-word-counts
#| echo: false
chronicle_article |>
  mutate(n_words = str_count(article, " ") + 1) |>
  group_by(column) |>
  summarize(avg_n_words = mean(n_words), n_articles = n()) |>
  knitr::kable(
    col.names = c("Column", "Avg. # words/article", "# articles"),
    digits = 0
  )
```

## Start with the URL for an first article

```{r}
#| message: false
chronicle <- read_csv("data/chronicle.csv")

article_url <- chronicle |>
  slice_head(n = 1) |>
  pull()

article_url
```

## Read in the page

```{r}
article_page <- read_html(article_url)

article_page
```

## Identify the elements you want

```
.article-content
```

![](images/13/article-content.png){fig-align="center"}

## Parse the elements you want - 1 {.smaller}

```{r}
#| eval: false
article_page |>
  html_elements(".article-content")
```

```{r}
#| echo: false
options(width = 500)
article_page |>
  html_elements(".article-content")
```

## Parse the elements you want - 2 {.smaller}

```{r}
article_page |>
  html_elements(".article-content") |>
  html_text2()
```

## Parse the elements you want - 3 {.smaller}

```{r}
article_page |>
  html_elements(".article-content") |>
  html_text2() |>
  str_remove_all("\n")
```

## Wrap in a function

```{r}
#| code-line-numbers: "|1|2|4-7"
parse_article_page <- function(url) {
  article_page <- read_html(url)

  article_page |>
    html_elements(".article-content") |>
    html_text2() |>
    str_remove_all("\n")
}
```

## Functions in R {.smaller}

```{r}
#| eval: false
function_name <- function(argument_1, argument_2, ...) {
  # what the function does
}
```

. . .

For example:

```{r}
multiply_by_3 <- function(x) {
  3 * x
}
```

<br>

. . .

```{r}
multiply_by_3(2)
multiply_by_3(10)
```

## Revisit: `parse_article_page()` {.smaller}

```{r}
#| code-line-numbers: "|1|2|4|5|6|7"
# fmt: skip
parse_article_page <- function(url) {    # define a function with one argument, url
  article_page <- read_html(url)         # read in the page at the url

  article_page |> # start with the page
    html_elements(".article-content") |> # extract element w/ selector .article-content
    html_text2() |>                      # extract text from element (and clean it up)
    str_remove_all("\n")                 # remove all newline characters, return result
}
```

## Test the function {.smaller}

```{r}
#| eval: false
chronicle |>
  slice_head(n = 1) |>
  mutate(article = parse_article_page(url)) |>
  select(title, article)
```

```{r}
#| echo: false
options(width = 200)
chronicle |>
  slice_head(n = 1) |>
  mutate(article = parse_article_page(url)) |>
  select(title, article)
```

## Test the function {.smaller}

```{r}
#| eval: false
#| code-line-numbers: "|3"
chronicle |>
  slice_head(n = 3) |>
  rowwise() |>
  mutate(article = parse_article_page(url)) |>
  select(title, article)
```

```{r}
#| echo: false
options(width = 200)
chronicle |>
  slice_head(n = 3) |>
  rowwise() |>
  mutate(article = parse_article_page(url)) |>
  select(title, article)
```

## Test the function {.smaller}

```{r}
#| eval: false
#| code-line-numbers: "|5"
chronicle |>
  slice_head(n = 3) |>
  rowwise() |>
  mutate(article = parse_article_page(url)) |>
  ungroup() |>
  select(title, article)
```

```{r}
#| echo: false
options(width = 200)
chronicle |>
  slice_head(n = 3) |>
  rowwise() |>
  mutate(article = parse_article_page(url)) |>
  ungroup() |>
  select(title, article)
```

## All articles {.smaller}

This can take a bit to run!

```{r}
#| eval: false
chronicle_article <- chronicle |>
  rowwise() |>
  mutate(article = parse_article_page(url)) |>
  ungroup()

chronicle_article
```

```{r}
#| echo: false
chronicle_article
```

## Revisit: Summary table

::: question
Now that you have the data, how would you produce the summary table below?
:::

```{r}
#| ref-label: chronicle-article-word-counts
#| echo: false
```

## Summarize 1 {.smaller}

```{r}
#| code-line-numbers: "|2"
chronicle_article |>
  mutate(n_words = str_count(article, " ") + 1) |>
  relocate(n_words)
```

## Summarize 2 {.smaller}

```{r}
#| code-line-numbers: "|3"
chronicle_article |>
  mutate(n_words = str_count(article, " ") + 1) |>
  group_by(column)
```

## Summarize 3 {.smaller}

```{r}
#| code-line-numbers: "|4-6"
chronicle_article |>
  mutate(n_words = str_count(article, " ") + 1) |>
  group_by(column) |>
  summarize(
    avg_n_words = mean(n_words)
  )
```

## Summarize 4 {.smaller}

```{r}
#| code-line-numbers: "|4-7"
chronicle_article |>
  mutate(n_words = str_count(article, " ") + 1) |>
  group_by(column) |>
  summarize(
    avg_n_words = mean(n_words),
    n_articles = n()
  )
```

## Make a pretty table {.smaller}

with the `kable()` function from the `knitr` package:

```{r}
#| code-line-numbers: "|8"
chronicle_article |>
  mutate(n_words = str_count(article, " ") + 1) |>
  group_by(column) |>
  summarize(
    avg_n_words = mean(n_words),
    n_articles = n()
  ) |>
  kable()
```

## Update column names {.smaller}

```{r}
#| code-line-numbers: "|9"
chronicle_article |>
  mutate(n_words = str_count(article, " ") + 1) |>
  group_by(column) |>
  summarize(
    avg_n_words = mean(n_words),
    n_articles = n()
  ) |>
  kable(
    col.names = c("Column", "Avg. # words/article", "# articles")
  )
```

## Update digits {.smaller}

```{r}
#| code-line-numbers: "|10"
chronicle_article |>
  mutate(n_words = str_count(article, " ") + 1) |>
  group_by(column) |>
  summarize(
    avg_n_words = mean(n_words),
    n_articles = n()
  ) |>
  kable(
    col.names = c("Column", "Avg. # words/article", "# articles"),
    digits = 0
  )
```
