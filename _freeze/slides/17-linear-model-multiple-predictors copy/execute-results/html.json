{
  "hash": "b14466ea57844ec4eeb193722f76fe39",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Model selection and overfitting\"\nsubtitle: \"Lecture 18\"\ndate: \"2025-10-28\"\nformat: \n  revealjs: \n    output-file: 18-model-selection-overfitting-slides.html\n    footer: \"[ðŸ”— sta199-f25.github.io](https://sta199-f25.github.io/)\"\n    theme: slides.scss\n    transition: fade\n    slide-number: true\n    logo: images/logo.png\n    pdf-separate-fragments: true\n    toc: false\n  html: \n    code-link: true\nfilters: \n  - ../remove-fmt-skip.lua\neditor_options: \n  chunk_output_type: console\n---\n\n\n\n# Warm-up\n\n## While you wait: Participate ðŸ“±ðŸ’» {.xsmall}\n\n:::::: columns\n:::: {.column width=\"70%\"}\n::: wooclap\n\nQUESTION\n\n::: wooclap-choices\n- CHOICES\n:::\n\n:::\n::::\n\n::: {.column width=\"30%\"}\n![](images/wooclap-qr.png){width=\"200\" fig-align=\"center\"}\n\n::: small\nScan the QR code or go to [app.wooclap.com/sta199](https://app.wooclap.com/sta199). Log in with your Duke NetID.\n:::\n\n:::\n::::::\n\n## Announcements {.smaller}\n\n-   Peer eval 2 due tonight at 11:59 pm\n\n-   ADD OTHER ANNOUNCEMENTS HERE\n\n# From last time\n\n## ae\\-12\\-penguins\\-model\\-multi {.smaller}\n\n::: appex\n-   Go to your ae project in RStudio.\n\n-   If you haven't yet done so, make sure all of your changes up to this point are committed and pushed, i.e., there's nothing left in your Git pane.\n\n-   If you haven't yet done so, click Pull to get today's application exercise file: *ae\\-12\\-penguins\\-model\\-multi\\.qmd*.\n\n-   Work through the application exercise in class, and render, commit, and push your edits.\n:::\n\n## What is the difference between $R^2$ and adjusted $R^2$?\n\n- $R^2$:\n  - Proportion of variability in the outcome explained by the model.\n  Useful for quantifying the fit of a given model.\n\n- Adjusted $R^2$:\n  - Proportion of variability in the outcome explained by the model, with a penalty added for the number of predictors in the model.\n  - Useful for comparing models.\n\n## Using adjusted $R^2$ to compare models\n\n- Fit two (or more) models to the same data set with different sets of predictors, selected based on subject-matter knowledge and/or exploratory data analysis results, compared their adjusted $R^2$ values, and selected the model with the highest adjusted $R^2$ value.\n\n- Perform stepwise selection (forward or backward) to select a model that maximizes adjusted $R^2$:\n  - Backward elimination: Start with a model that includes all candidate predictors, and iteratively remove the predictor that results in the largest increase in adjusted $R^2$ until no further improvement is observed.\n  - Forward selection: Start with a model that includes no predictors, and iteratively add the predictor that results in the largest increase in adjusted $R^2$ until no further improvement is observed.\n\n## Using other criteria to compare models {.smaller}\n\nSame model selection approaches (subject-matter knowledge, stepwise selection) can be used with various model comparison criteria, such as:\n\n- Maximize adjusted $R^2$\n- _Minimize Akaike Information Criterion (AIC)_\n- _Minimize Bayesian Information Criterion (BIC)_\n- ...\n\n::: callout-note\nOnly mentioning criteria other than adjusted $R^2$ here to build awareness that other criteria exist. We will not cover all of them in detail in this course (and this is not even a complete list), but you will encounter them in higher-level modeling courses and in practice.\n:::\n\n## Balancing fit and complexity\n\n- More complex models (i.e., models with more predictors) tend to fit the data at hand better, but may not generalize well to new data.\n\n- Model selection criteria, like adjusted $R^2$, help balance model fit and complexity to avoid overfitting by penalizing models with more predictors.\n\n## Overfitting {.smaller}\n\n- Overfitting occurs when a model captures not only the underlying relationship between predictors and outcome but also the random noise in the data.\n\n- Overfitted models tend to perform well on the observed data but poorly on new, unseen data.\n  - Good news: We have techniques to detect and prevent overfitting.\n  - Bad news: We won't get into those until next week.\n  - For today, we'll discuss one feature of data that can lead to overfitting -- outliers.\n\n# Outliers\n\n## Outliers in regression\n\n::: incremental\n- Outliers are observations that fall far from the main cloud of points.\n\n- They can be outlying in:\n  - the $x$ direction,\n  - the $y$ direction, or\n  - both.\n\n- However, being outlying in a univariate sense does **not** always mean being outlying from the **bivariate model**.\n\n- Points that are in-line with the bivariate model usually do **not** influence the least squares line, even if they are extreme in $x$, $y$, or both.\n:::\n\n## Outliers and influence\n\n::: columns\n::: {.column width=55%}\n#| label: outliers-influence-1\n#| echo: false\n#| message: false\n#| fig-asp: 0.7\n#| fig-width: 10\nd1 <- simulated_scatter |>\n  filter(group == 24) |>\n  mutate(outlier = if_else(y == min(y), TRUE, FALSE))\nd2 <- simulated_scatter |>\n  filter(group == 25) |>\n  mutate(outlier = if_else(y == min(y), TRUE, FALSE))\nd3 <- simulated_scatter |>\n  filter(group == 26) |>\n  mutate(outlier = if_else(y == max(y), TRUE, FALSE))\n\nm1_aug <- augment(lm(y ~ x, data = d1)) |>\n  mutate(outlier = if_else(y == min(y), TRUE, FALSE))\n\nm2_aug <- augment(lm(y ~ x, data = d2)) |>\n  mutate(outlier = if_else(y == min(y), TRUE, FALSE))\n\nm3_aug <- augment(lm(y ~ x, data = d3)) |>\n  mutate(outlier = if_else(y == max(y), TRUE, FALSE))\n\np_1 <- ggplot(d1, aes(x = x, y = y)) +\n  geom_point(size = 2, alpha = 0.8) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  geom_point(\n    data = d1 |> filter(outlier),\n    size = 5,\n    shape = \"circle open\",\n    color = IMSCOL[\"red\", \"full\"],\n    stroke = 2\n  ) +\n  labs(title = \"A\") +\n  theme(\n    panel.grid = element_blank(),\n    axis.text = element_blank(),\n    panel.border = element_rect(colour = \"gray\", fill = NA, linewidth = 1)\n  ) +\n  scale_x_continuous(expand = expansion(mult = 0.12)) +\n  scale_y_continuous(expand = expansion(mult = 0.12))\n\n\np_1_res <- ggplot(m1_aug, aes(x = .fitted, y = .resid)) +\n  geom_point(alpha = 0.7, size = 2) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  geom_point(\n    data = m1_aug |> filter(outlier),\n    size = 5,\n    shape = \"circle open\",\n    color = IMSCOL[\"red\", \"full\"],\n    stroke = 2\n  ) +\n  labs(x = \"Predicted y\", y = \"Residual\") +\n  theme(\n    panel.grid = element_blank(),\n    axis.text = element_blank(),\n    panel.border = element_rect(colour = \"gray\", fill = NA, linewidth = 1)\n  ) +\n  scale_x_continuous(expand = expansion(mult = 0.12)) +\n  scale_y_continuous(limits = c(-8, 8), expand = expansion(mult = 0.12))\n\np_2 <- ggplot(d2, aes(x = x, y = y)) +\n  geom_point(size = 2, alpha = 0.8) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  geom_point(\n    data = d2 |> filter(outlier),\n    size = 5,\n    shape = \"circle open\",\n    color = IMSCOL[\"green\", \"full\"],\n    stroke = 2\n  ) +\n  labs(title = \"B\") +\n  theme(\n    panel.grid = element_blank(),\n    axis.text = element_blank(),\n    panel.border = element_rect(colour = \"gray\", fill = NA, linewidth = 1)\n  ) +\n  scale_x_continuous(expand = expansion(mult = 0.12)) +\n  scale_y_continuous(expand = expansion(mult = 0.12))\n\np_2_res <- ggplot(m2_aug, aes(x = .fitted, y = .resid)) +\n  geom_point(size = 2, alpha = 0.8) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  geom_point(\n    data = m2_aug |> filter(outlier),\n    size = 5,\n    shape = \"circle open\",\n    color = IMSCOL[\"green\", \"full\"],\n    stroke = 2\n  ) +\n  labs(x = \"Predicted y\", y = \"Residual\") +\n  theme(\n    panel.grid = element_blank(),\n    axis.text = element_blank(),\n    panel.border = element_rect(colour = \"gray\", fill = NA, linewidth = 1)\n  ) +\n  scale_x_continuous(expand = expansion(mult = 0.12)) +\n  scale_y_continuous(limits = c(-8, 8), expand = expansion(mult = 0.12))\n\np_3 <- ggplot(d3, aes(x = x, y = y)) +\n  geom_point(size = 2, alpha = 0.8) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  geom_point(\n    data = d3 |> filter(outlier),\n    size = 5,\n    shape = \"circle open\",\n    color = IMSCOL[\"pink\", \"full\"],\n    stroke = 2\n  ) +\n  labs(title = \"C\") +\n  theme(\n    panel.grid = element_blank(),\n    axis.text = element_blank(),\n    panel.border = element_rect(colour = \"gray\", fill = NA, linewidth = 1)\n  ) +\n  scale_x_continuous(expand = expansion(mult = 0.12)) +\n  scale_y_continuous(expand = expansion(mult = 0.12))\n\np_3_res <- ggplot(m3_aug, aes(x = .fitted, y = .resid)) +\n  geom_point(size = 2, alpha = 0.8) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  geom_point(\n    data = m3_aug |> filter(outlier),\n    size = 5,\n    shape = \"circle open\",\n    color = IMSCOL[\"pink\", \"full\"],\n    stroke = 2\n  ) +\n  labs(x = \"Predicted y\", y = \"Residual\") +\n  theme(\n    panel.grid = element_blank(),\n    axis.text = element_blank(),\n    panel.border = element_rect(colour = \"gray\", fill = NA, linewidth = 1)\n  ) +\n  scale_x_continuous(expand = expansion(mult = 0.12)) +\n  scale_y_continuous(limits = c(-8, 8), expand = expansion(mult = 0.12))\n\np_1 +\n  theme(plot.margin = unit(c(0, 10, 5, 0), \"pt\")) +\n  p_2 +\n  theme(plot.margin = unit(c(0, 10, 5, 0), \"pt\")) +\n  p_3 +\n  p_1_res +\n  theme(plot.margin = unit(c(0, 10, 5, 0), \"pt\")) +\n  p_2_res +\n  theme(plot.margin = unit(c(0, 10, 5, 0), \"pt\")) +\n  p_3_res +\n  plot_layout(ncol = 3, heights = c(2, 1))\n``\n`\n:::\n::: {.column width=45%}\n::: incremental\n- **A:** One outlier in the $y$ direction, also outlying in the bivariate model; slightly influences the regression line.\n- **B:** One outlier on the right (outlying in $x$ and $y$, but not outlying in the bivariate model); close to the regression line and not influential.\n- **C:** One point far from the cloud (outlying in $x$, $y$, and bivariate model); pulls the regression line upward, worsening fit for the main data.\n:::\n:::\n:::\n\n## Outliers and influence\n\n::: columns\n::: {.column width=55%}\n#| label: outliers-influence-2\n#| echo: false\n#| warning: false\n#| fig-asp: 0.7\n#| fig-width: 10\nd4 <- simulated_scatter |>\n  filter(group == 27)\n\nd5 <- simulated_scatter |> filter(group == 28)\nd6 <- simulated_scatter |> filter(group == 29)\n\nm4_aug <- augment(lm(y ~ x, data = d4))\nm5_aug <- augment(lm(y ~ x, data = d5))\nm6_aug <- augment(lm(y ~ x, data = d6))\n\np_4 <- ggplot(d4, aes(x = x, y = y)) +\n  geom_point(size = 2, alpha = 0.7) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  theme(\n        panel.grid = element_blank(),\n        axis.text = element_blank(),\n        panel.border = element_rect(colour = \"gray\", fill = NA, linewidth = 1)\n    ) +\n  labs(title = \"D\")\n\np_4_res <- ggplot(m4_aug, aes(x = .fitted, y = .resid)) +\n  geom_point(size = 2, alpha = 0.7) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  labs(x = \"Predicted y\", y = \"Residual\") +\n  theme(\n        panel.grid = element_blank(),\n        axis.text = element_blank(),\n        panel.border = element_rect(colour = \"gray\", fill = NA, linewidth = 1)\n    ) +\n  ylim(-4, 4)\n\np_5 <- ggplot(d5, aes(x = x, y = y)) +\n  geom_point(size = 2, alpha = 0.7) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  theme(\n        panel.grid = element_blank(),\n        axis.text = element_blank(),\n        panel.border = element_rect(colour = \"gray\", fill = NA, linewidth = 1)\n    ) +\n  labs(title = \"E\")\n\np_5_res <- ggplot(m5_aug, aes(x = .fitted, y = .resid)) +\n  geom_point(alpha = 0.7, size = 2) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  labs(x = \"Predicted y\", y = \"Residual\") +\n  theme(\n        panel.grid = element_blank(),\n        axis.text = element_blank(),\n        panel.border = element_rect(colour = \"gray\", fill = NA, linewidth = 1)\n    ) +\n  ylim(-4, 4)\n\np_6 <- ggplot(d6, aes(x = x, y = y)) +\n  geom_point(size = 2, alpha = 0.7) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  theme(\n        panel.grid = element_blank(),\n        axis.text = element_blank(),\n        panel.border = element_rect(colour = \"gray\", fill = NA, linewidth = 1)\n    ) +\n  labs(title = \"F\")\n\np_6_res <- ggplot(m6_aug, aes(x = .fitted, y = .resid)) +\n  geom_point(size = 2, alpha = 0.7) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  labs(x = \"Predicted y\", y = \"Residual\") +\n  theme(\n        panel.grid = element_blank(),\n        axis.text = element_blank(),\n        panel.border = element_rect(colour = \"gray\", fill = NA, linewidth = 1)\n    ) +\n  ylim(-4, 4)\n\np_4 + theme(plot.margin = unit(c(0, 10, 5, 0), \"pt\")) +\n  p_5 + theme(plot.margin = unit(c(0, 10, 5, 0), \"pt\")) + p_6 +\n  p_4_res + theme(plot.margin = unit(c(0, 10, 5, 0), \"pt\")) +\n  p_5_res + theme(plot.margin = unit(c(0, 10, 5, 0), \"pt\")) + p_6_res +\n  plot_layout(ncol = 3, heights = c(2, 1))\n`\n``\n:::\n::: {.column width=45%}\n::: incremental\n- **D:** A secondary small cloud of four points (outlying in $x$ and bivariate model); strongly influences the regression line, creating poor fit.\n- **E:** Outlier far right (outlying in $x$ and $y$); the regression line is largely controlled by this single point, imposing a trend where there is none.\n- **F:** One outlier far away (outlying in $x$ and $y$), but in-line with the model; has little influence.\n:::\n:::\n:::\n\n## Types of outliers\n\n::: incremental\n- **Outliers:** Points or groups of points that stand out from the rest of the data.\n\n- **Leverage points:** Points that fall horizontally away from the center of the cloud tend to pull harder on the line, so we call them points with **high leverage** or **leverage points**.\n\n- **Influential points:** Outliers, generally high leverage points, that actually alter the slope or position of the regression line.\n  - We say a point is influential if omitting it would substantially change the regression model.\n:::\n\n## Practical advice\n\n::: incremental\n- Test your analysis **with and without** outliers.\n\n- Compare and discuss the impact of outliers on model fit.\n\n- Present both models to stakeholders to choose the most reasonable interpretation.\n:::\n\n. . .\n\n::: columns\n::: {.column width=70%}\n::: callout-warning\nRemoving outliers should only be done with strong justification -- excluding interesting or extreme cases can lead to misleading models, poor predictive performance, and flawed conclusions.\n:::\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}