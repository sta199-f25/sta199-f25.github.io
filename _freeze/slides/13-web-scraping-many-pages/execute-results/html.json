{
  "hash": "341e3b5739b9efeb41433127233c96c1",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Web scraping<br>many pages\"\nsubtitle: \"Lecture 14\"\ndate: \"2025-10-09\"\nformat: \n  revealjs: \n    output-file: 13-web-scraping-many-pages-slides.html\n    footer: \"[ðŸ”— sta199-f25.github.io](https://sta199-f25.github.io/)\"\n    theme: slides.scss\n    transition: fade\n    slide-number: true\n    logo: images/logo.png\n    pdf-separate-fragments: true\n    toc: false\n  html: \n    code-link: true\nfilters: \n  - ../remove-fmt-skip.lua\neditor_options: \n  chunk_output_type: console\n---\n\n\n\n# Warm-up\n\n## While you wait: Participate ðŸ“±ðŸ’» {.xsmall}\n\n::: columns\n\n::: {.column width=\"70%\"}\n\n::: wooclap\n\nThe following code in `chronicle-scrape.R` extracts titles of an opinion article from The Chronicle website:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npage <- read_html(\n  \"https://www2.stat.duke.edu/~cr173/data/dukechronicle-opinion/www.dukechronicle.com/section/opinionabc4.html\"\n)\n\ntitles <- page |>\n  html_elements(\".space-y-4 .font-extrabold\") |>\n  html_text()\n```\n:::\n\n\nWhich of the following needs to change to extract column titles instead?\n\n::: wooclap-choices\n- Change the URL in `read_html()`\n- Change the function `html_elements()` to `html_element()`\n- Change the CSS selector `.space-y-4 .font-extrabold` to `.space-y-4 .text-brand`\n- Change the function `html_text()` to `html_attr()`\n:::\n\n:::\n\n:::\n\n::: {.column width=\"30%\"}\n![](images/wooclap-qr.png){width=\"200\" fig-align=\"center\"}\n\n::: small\nScan the QR code or go to [app.wooclap.com/sta199](https://app.wooclap.com/sta199). Log in with your Duke NetID.\n:::\n\n:::\n\n:::\n\n## Announcements {.smaller}\n\n- HW 2, Question 7: Reproduce the colorful box plot -- We caught an error in grading (any theme with a white background would have worked). If you originally missed points due to not using `theme_bw()`, but you used another theme with a white background, we've updated your grade.\n\n- Midsemester course survey due tonight at 11:59pm\n\n- Project proposals (Milestone 2) + first peer evaluation due next Thursday at 11:59pm -- any questions?\n\n# From last time\n\n## Opinion articles in The Chronicle {.smaller}\n\nGo to <https://www2.stat.duke.edu/~cr173/data/dukechronicle-opinion/www.dukechronicle.com/section/opinionabc4.html> (copy of The Chronicle opinion section as of October 7, 2025).\n\n## Goal\n\n::::: columns\n::: {.column width=\"50%\"}\n-   Scrape data and organize it in a tidy format in R\n-   Perform light text parsing to clean data\n-   Summarize and visualze the data\n:::\n\n::: {.column width=\"50%\"}\n![](images/13/chronicle-data.png){fig-align=\"center\"}\n:::\n:::::\n\n## ae\\-09\\-chronicle\\-scrape {.smaller}\n\n::: appex\n-   Go to your ae project in RStudio.\n\n-   If you haven't yet done so, make sure all of your changes up to this point are committed and pushed, i.e., there's nothing left in your Git pane.\n\n-   If you haven't yet done so, click Pull to get today's application exercise file: *ae\\-09\\-chronicle\\-scrape\\.qmd* and *`chronicle-scrape.R`*.\n:::\n\n## Participate ðŸ“±ðŸ’» {.xsmall}\n\n::: columns\n\n::: {.column width=\"70%\"}\n\n::: wooclap\n\nPut the folllowing tasks in order to scrape data from a website:\n\n::: wooclap-choices\n- Use the SelectorGadget identify tags for elements you want to grab\n- Use `read_html()` to read the page's source code into R\n- Use other functions from the **rvest** package to parse the elements you're interested in\n- Put the components together in a data frame (a tibble) and analyze it like you analyze any other data\n:::\n\n:::\n\n:::\n\n::: {.column width=\"30%\"}\n![](images/wooclap-qr.png){width=\"200\" fig-align=\"center\"}\n\n::: small\nScan the QR code or go to [app.wooclap.com/sta199](https://app.wooclap.com/sta199). Log in with your Duke NetID.\n:::\n\n:::\n\n:::\n\n## A new R workflow {.smaller}\n\n-   When working in a Quarto document, your analysis is re-run each time you knit\n\n-   If web scraping in a Quarto document, you'd be re-scraping the data each time you knit, which is undesirable (and not *nice*)!\n\n-   An alternative workflow:\n\n    -   Use an R script to save your code\n    -   Saving interim data scraped using the code in the script as CSV or RDS files\n    -   Use the saved data in your analysis in your Quarto document\n",
    "supporting": [
      "13-web-scraping-many-pages_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}