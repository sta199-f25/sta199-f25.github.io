{
  "hash": "6bfac69e01cb620295fa04b0ca61205e",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Logistic regression\"\nsubtitle: \"Lecture 20\"\ndate: \"2025-11-06\"\nformat: \n  revealjs: \n    output-file: 20-logistic-regression-slides.html\n    footer: \"[ðŸ”— sta199-f25.github.io](https://sta199-f25.github.io/)\"\n    theme: slides.scss\n    transition: fade\n    slide-number: true\n    logo: images/logo.png\n    pdf-separate-fragments: true\n    toc: false\n  html: \n    code-link: true\nfilters: \n  - ../remove-fmt-skip.lua\neditor_options: \n  chunk_output_type: console\n---\n\n\n\n# Warm-up\n\n## Announcements {.smaller}\n\n-   My Friday office hours extended -- 12:45 - 3:00 PM in for this week\n\n-   Project presentations: Hard stop at 5 minute mark! No limit on number of slides, but be mindful of how long it takes you to go through each.\n\n## Project questions {.scrollable .smaller}\n\n::: incremental\n- Focus: Can expand focus after research question, sure!\n\n- Citations: Don't have to have them on slides and don't need to say them out loud unless relevant to presentation, must include in writeup.\n\n- Vairble names: It's ok to have some long variable names, but if you're using them a lot in your code, make sure code is easy to follow.\n\n- Outliers: Evaluate if they're genuinely influencing your model.\n\n- Grading: Rubric is in the milestone 6.\n\n- Categorical variables + summary stats: Correlation isn't appropriate, report %s or make stacked bar plots.\n\n- There is no single correct number of graphs, tables, etc.\n\n- Review your website by clicking on the link from your repo.\n\n- Analysis writeup can be broken into multiple pieces with plots, tables, etc. sprinkled.\n\n- Who is grading? TAs and myself, I'll be at (most of) the presentations.\n\n- When is it due?\n:::\n\n# Logistic regression\n\n## Packages\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse) # data wrangling and visualization\nlibrary(tidymodels) # modeling\nlibrary(openintro) # emails data\nlibrary(fivethirtyeight) # movies data\nlibrary(palmerpenguins) # penguins data\nlibrary(ggthemes) # accessible color palettes\n```\n:::\n\n\n## Thus far...\n\nWe have been studying regression:\n\n-   What combinations of data types have we seen?\n\n-   What did the picture look like?\n\n## Recap: Simple linear regression\n\nNumerical outcome and one numerical predictor:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](20-logistic-regression_files/figure-html/movies-1.png){width=672}\n:::\n:::\n\n\n## Recap: Simple linear regression\n\nNumerical outcome and one categorical predictor (two levels):\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](20-logistic-regression_files/figure-html/card-krueger-1.png){width=672}\n:::\n:::\n\n\n## Recap: Multiple linear regression\n\nNumerical outcome, numerical and categorical predictors:\n\n\n::: {.cell layout-ncol=\"2\"}\n::: {.cell-output-display}\n![](20-logistic-regression_files/figure-html/penguins-additive-interaction-1.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](20-logistic-regression_files/figure-html/penguins-additive-interaction-2.png){width=672}\n:::\n:::\n\n\n## Today: a *binary* outcome {.smaller}\n\n\n::: {.cell}\n\n:::\n\n\n$$\ny = \n\\begin{cases}\n1 & &&\\text{eg. Yes, Win, True, Heads, Success}\\\\\n0 & &&\\text{eg. No, Lose, False, Tails, Failure}.\n\\end{cases}\n$$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](20-logistic-regression_files/figure-html/binary-outcome-scatter-1.png){width=960}\n:::\n:::\n\n\n## Who cares?\n\nIf we can model the relationship between predictors ($x$) and a binary outcome ($y$), we can use the model to do a special kind of prediction called *classification*.\n\n## Example: Is the e-mail spam or not? {.smaller}\n\n$$\n\\mathbf{x}: \\text{word and character counts in an e-mail.}\n$$\n\n![](images/20/spam.png){fig-align=\"center\" width=\"70%\"}\n\n$$\ny\n= \n\\begin{cases}\n1 & \\text{it's spam}\\\\\n0 & \\text{it's legit}\n\\end{cases}\n$$\n\n## Example: Is it cancer or not? {.smaller}\n\n$$\n\\mathbf{x}: \\text{features in a medical image.}\n$$\n\n![](images/20/head-neck.jpg){fig-align=\"center\"}\n\n$$\ny\n= \n\\begin{cases}\n1 & \\text{it's cancer}\\\\\n0 & \\text{it's healthy}\n\\end{cases}\n$$\n\n## Example: Will they default? {.smaller}\n\n$$\n\\mathbf{x}: \\text{financial and demographic info about a loan applicant.}\n$$\n\n![](images/20/fico.jpg){fig-align=\"center\" width=\"60%\"}\n\n$$\ny\n= \n\\begin{cases}\n1 & \\text{applicant is at risk of defaulting on loan}\\\\\n0 & \\text{applicant is safe}\n\\end{cases}\n$$\n\n## How do we model this type of data?\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](20-logistic-regression_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\n## Straight line of best fit is a little silly\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](20-logistic-regression_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n## Instead: S-curve of best fit {.smaller}\n\nInstead of modeling $y$ directly, we model the probability that $y=1$:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](20-logistic-regression_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n-   \"Given new email, what's the probability that it's spam?''\n-   \"Given new image, what's the probability that it's cancer?''\n-   \"Given new loan application, what's the probability that they default?''\n\n## Why don't we model y directly?\n\n-   **Recall regression with a numerical outcome**:\n\n    -   Our models do not output *guarantees* for $y$, they output predictions that describe behavior *on average*\n\n-   **Similar when modeling a binary outcome**:\n\n    -   Our models cannot directly guarantee that $y$ will be zero or one. The correct analog to \"on average\" for a 0/1 outcome is \"what's the probability?\"\n\n## So, what is this S-curve, anyway?\n\nIt's the *logistic function*:\n\n$$\n\\text{Prob}(y = 1)\n=\n\\frac{e^{\\beta_0+\\beta_1x}}{1+e^{\\beta_0+\\beta_1x}}.\n$$\n\nIf you set $p = \\text{Prob}(y = 1)$ and do some algebra, you get the simple linear model for the *log-odds*:\n\n$$\n\\log\\left(\\frac{p}{1-p}\\right)\n=\n\\beta_0+\\beta_1x.\n$$\n\nThis is called the *logistic regression* model.\n\n## Log-odds?\n\n-   $p = \\text{Prob}(y = 1)$ is a probability. A number between 0 and 1\n\n-   $p / (1 - p)$ is the odds. A number between 0 and $\\infty$\n\n> \"The odds of this lecture going well are 10 to 1.\"\n\n-   The log odds $\\log(p / (1 - p))$ is a number between $-\\infty$ and $\\infty$, which is suitable for the linear model.\n\n## Probability to odds\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](20-logistic-regression_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n## Odds to log odds\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](20-logistic-regression_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n## Participate ðŸ“±ðŸ’» {.xsmall}\n\n:::::: columns\n:::: {.column width=\"70%\"}\n::: wooclap\n\nIf $p$ is the probability of success, what is the following called: \n\n$$ \\frac{p}{1-p} $$\n\n::: wooclap-choices\n- Probability of failure\n- Odds of failure\n- Odds of success\n- Log-odds of success\n:::\n\n:::\n::::\n\n::: {.column width=\"30%\"}\n![](images/wooclap-qr.png){width=\"200\" fig-align=\"center\"}\n\n::: small\nScan the QR code or go to [app.wooclap.com/sta199](https://app.wooclap.com/sta199). Log in with your Duke NetID.\n:::\n\n:::\n::::::\n\n## Logistic regression\n\n$$\n\\log\\left(\\frac{p}{1-p}\\right)\n=\n\\beta_0+\\beta_1x.\n$$\n\n-   The *logit* function $\\log(p / (1-p))$ is an example of a *link function* that transforms the linear model to have an appropriate range\n\n-   This is an example of a *generalized linear model*\n\n## Estimation\n\n-   We estimate the parameters $\\beta_0$ (intercept) and $\\beta_1$ (slope) using *maximum likelihood* (don't worry about it) to get the \"best fitting\" S-curve\n\n-   The fitted model is\n\n$$\n\\log\\left(\\frac{\\widehat{p}}{1-\\widehat{p}}\\right)\n=\nb_0+b_1x.\n$$\n\n## Today's data {.smaller}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nemail |>\n  select(c(spam, dollar, viagra, winner, password, exclaim_mess)) |>\n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 3,921\nColumns: 6\n$ spam         <fct> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,â€¦\n$ dollar       <dbl> 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0,â€¦\n$ viagra       <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,â€¦\n$ winner       <fct> no, no, no, no, no, no, no, no, no, no, no, no,â€¦\n$ password     <dbl> 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,â€¦\n$ exclaim_mess <dbl> 0, 1, 6, 48, 1, 1, 1, 18, 1, 0, 2, 1, 0, 10, 4,â€¦\n```\n\n\n:::\n:::\n\n\n## Fitting a logistic model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlogistic_fit <- logistic_reg() |>\n  fit(spam ~ exclaim_mess, data = email)\n\ntidy(logistic_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 Ã— 5\n  term          estimate std.error statistic p.value\n  <chr>            <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)  -2.27      0.0553     -41.1     0    \n2 exclaim_mess  0.000272  0.000949     0.287   0.774\n```\n\n\n:::\n:::\n\n\nFitted equation for the log-odds:\n\n$$\n\\log\\left(\\frac{\\hat{p}}{1-\\hat{p}}\\right)\n=\n-2.27\n+\n0.000272\\times exclaim~mess\n$$\n\n## Interpreting the intercept\n\nIf `exclaim_mess = 0`, then\n\n$$\n\\hat{p}=\\widehat{P(y=1)}=\\frac{e^{-2.27}}{1+e^{-2.27}}\\approx 0.09.\n$$\n\nSo, an email with no exclamation marks has a 9% chance of being spam.\n\n## Interpreting the slope is tricky {.smaller .scrollable}\n\nRecall:\n\n$$\n\\log\\left(\\frac{\\widehat{p}}{1-\\widehat{p}}\\right)\n=\nb_0+b_1x.\n$$\n\n. . .\n\nAlternatively:\n\n$$\n\\frac{\\widehat{p}}{1-\\widehat{p}}\n=\ne^{b_0+b_1x}\n=\n\\color{blue}{e^{b_0}e^{b_1x}}\n.\n$$\n\n. . .\n\nIf $x$ is higher by one unit, we have:\n\n$$\n\\frac{\\widehat{p}}{1-\\widehat{p}}\n=\ne^{b_0}e^{b_1(x+1)}\n=\ne^{b_0}e^{b_1x+b_1}\n=\n{\\color{blue}{e^{b_0}e^{b_1x}}}{\\color{red}{e^{b_1}}}\n.\n$$\n\n. . .\n\nA one unit increase in $x$ is associated with a change in odds by a factor of $e^{b_1}$. Helpful! ðŸ™„\n\n## Back to the example...\n\n$$\n\\log\\left(\\frac{\\hat{p}}{1-\\hat{p}}\\right)\n=\n-2.27\n+\n0.000272\\times exclaim~mess\n$$\n\nEmails with one additional exclamation point are predicted to have odds of being spam that are **higher** by a factor of $e^{0.000272}\\approx 1.000272$, on average.\n\n# Classification <br> (logistic regression by another name...)\n\n## Step 1: fit the model {.smaller}\n\nSelect a number $0 < p^* < 1$:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](20-logistic-regression_files/figure-html/step-0-1.png){width=960}\n:::\n:::\n\n\n-   if $\\text{Prob}(y=1)\\leq p^*$, then predict $\\widehat{y}=0$\n-   if $\\text{Prob}(y=1)> p^*$, then predict $\\widehat{y}=1$.\n\n## Step 2: pick a threshold {.smaller}\n\nSelect a number $0 < p^* < 1$:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](20-logistic-regression_files/figure-html/step-1-1.png){width=960}\n:::\n:::\n\n\n-   if $\\text{Prob}(y=1)\\leq p^*$, then predict $\\widehat{y}=0$\n-   if $\\text{Prob}(y=1)> p^*$, then predict $\\widehat{y}=1$.\n\n## Step 3: find the \"decision boundary\" {.smaller}\n\nSolve for the x-value that matches the threshold:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](20-logistic-regression_files/figure-html/step-2-1.png){width=672}\n:::\n:::\n\n\n-   if $\\text{Prob}(y=1)\\leq p^*$, then predict $\\widehat{y}=0$\n-   if $\\text{Prob}(y=1)> p^*$, then predict $\\widehat{y}=1$.\n\n## Step 4: classify a new arrival {.smaller}\n\nA new person shows up with $x_{\\text{new}}$. Which side of the boundary are they on?\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](20-logistic-regression_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n-   if $x_{\\text{new}} \\leq x^\\star$, then $\\text{Prob}(y=1)\\leq p^*$, so predict $\\widehat{y}=0$ for the new person\n-   if $x_{\\text{new}} > x^\\star$, then $\\text{Prob}(y=1)> p^*$, so predict $\\widehat{y}=1$ for the new person.\n\n## Let's change the threshold {.smaller}\n\nA new person shows up with $x_{\\text{new}}$. Which side of the boundary are they on?\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](20-logistic-regression_files/figure-html/lower-threshold-1.png){width=672}\n:::\n:::\n\n\n-   if $x_{\\text{new}} \\leq x^\\star$, then $\\text{Prob}(y=1)\\leq p^*$, so predict $\\widehat{y}=0$ for the new person\n-   if $x_{\\text{new}} > x^\\star$, then $\\text{Prob}(y=1)> p^*$, so predict $\\widehat{y}=1$ for the new person.\n\n## Let's change the threshold {.smaller}\n\nA new person shows up with $x_{\\text{new}}$. Which side of the boundary are they on?\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](20-logistic-regression_files/figure-html/higher-threshold-1.png){width=672}\n:::\n:::\n\n\n-   if $x_{\\text{new}} \\leq x^\\star$, then $\\text{Prob}(y=1)\\leq p^*$, so predict $\\widehat{y}=0$ for the new person\n-   if $x_{\\text{new}} > x^\\star$, then $\\text{Prob}(y=1)> p^*$, so predict $\\widehat{y}=1$ for the new person.\n\n## Nothing special about one predictor... {.smaller}\n\nTwo numerical predictors and one binary outcome:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](20-logistic-regression_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n## \"Multiple\" logistic regression\n\nOn the probability scale:\n\n$$\n\\text{Prob}(y = 1)\n=\n\\frac{e^{\\beta_0+\\beta_1x_1+\\beta_2x_2+...+\\beta_mx_m}}{1+e^{\\beta_0+\\beta_1x_1+\\beta_2x_2+...+\\beta_mx_m}}.\n$$\n\nFor the log-odds, a *multiple* linear regression:\n\n$$\n\\log\\left(\\frac{p}{1-p}\\right)\n=\n\\beta_0+\\beta_1x_1+\\beta_2x_2+...+\\beta_mx_m.\n$$\n\n## Decision boundary, again {.smaller}\n\nIt's linear! Consider two numerical predictors:\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](20-logistic-regression_files/figure-html/p-thresh-50-1.png){width=672}\n:::\n:::\n\n\n-   if new $(x_1,\\,x_2)$ below, $\\text{Prob}(y=1)\\leq p^*$ $\\rightarrow$ predict $\\widehat{y}=0$ for new observation\n-   if new $(x_1,\\,x_2)$ above, $\\text{Prob}(y=1)> p^*$ $\\rightarrow$ predict $\\widehat{y}=1$ for new observation\n\n## Decision boundary, again {.smaller}\n\nIt's linear! Consider two numerical predictors:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](20-logistic-regression_files/figure-html/p-thresh-15-1.png){width=672}\n:::\n:::\n\n\n-   if new $(x_1,\\,x_2)$ below, $\\text{Prob}(y=1)\\leq p^*$ $\\rightarrow$ predict $\\widehat{y}=0$ for new observation\n-   if new $(x_1,\\,x_2)$ above, $\\text{Prob}(y=1)> p^*$ $\\rightarrow$ predict $\\widehat{y}=1$ for new observation\n\n## Decision boundary, again {.smaller}\n\nIt's linear! Consider two numerical predictors:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](20-logistic-regression_files/figure-html/p-thresh-90-1.png){width=672}\n:::\n:::\n\n\n-   if new $(x_1,\\,x_2)$ below, $\\text{Prob}(y=1)\\leq p^*$ $\\rightarrow$ predict $\\widehat{y}=0$ for new observation\n-   if new $(x_1,\\,x_2)$ above, $\\text{Prob}(y=1)> p^*$ $\\rightarrow$ predict $\\widehat{y}=1$ for new observation\n\n## The classifier isn't perfect {.smaller}\n\nThere are blue points in the orange region and oranges in the blue:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](20-logistic-regression_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n## The classifier isn't perfect {.smaller}\n\n[Blue points in the orange region: spam (1) emails misclassified as legit (0)]{style=\"color: red;\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](20-logistic-regression_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n## The classifier isn't perfect {.smaller}\n\n[Orange points in the blue region: legit (0) emails misclassified as spam (1)]{style=\"color: red;\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](20-logistic-regression_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\n\n## How do you pick the threshold? {.smaller}\n\nTo balance out the two kinds of errors:\n\n![](images/20/confusion-matrix.png)\n\n-   High threshold \\>\\> Hard to classify as 1 \\>\\> FP less likely, FN more likely\n-   Low threshold \\>\\> Easy to classify as 1 \\>\\> FP more likely, FN less likely\n\n## Silly examples\n\n-   Set p\\* = 0\n\n    -   Classify every email as spam (1)\n    -   No false negatives, but *a lot* of false positives\n\n-   Set p\\* = 1\n\n    -   Classify every email as legit (0)\n    -   No false positives, but *a lot* of false negatives.\n\nYou pick a threshold in between to strike a balance. The exact number depends on context.\n\n## ae\\-13\\-spam\\-filter\n\n::: appex\n-   Go to your ae project in RStudio.\n\n-   If you haven't yet done so, make sure all of your changes up to this point are committed and pushed, i.e., there's nothing left in your Git pane.\n\n-   If you haven't yet done so, click Pull to get today's application exercise file: *ae\\-13\\-spam\\-filter\\.qmd*.\n\n-   Work through the application exercise in class, and render, commit, and push your edits.\n:::\n",
    "supporting": [
      "20-logistic-regression_files/figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}