{
  "hash": "1a9670ef965842ce90b19a96da5aadca",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Model selection and overfitting\"\nsubtitle: \"Lecture 18\"\ndate: \"2025-10-30\"\nformat: \n  revealjs: \n    output-file: 18-model-selection-overfitting-slides.html\n    footer: \"[ðŸ”— sta199-f25.github.io](https://sta199-f25.github.io/)\"\n    theme: slides.scss\n    transition: fade\n    slide-number: true\n    logo: images/logo.png\n    pdf-separate-fragments: true\n    toc: false\n  html: \n    code-link: true\nfilters: \n  - ../remove-fmt-skip.lua\neditor_options: \n  chunk_output_type: console\n---\n\n\n\n# Warm-up\n\n## While you wait\n\n::: task\n\n::: columns\n\n:::: {.column width=\"70%\"}\n\nRead the NY Times story \n\n[How China Raced Ahead of the U.S. on Nuclear Power](https://www.nytimes.com/interactive/2025/10/22/climate/china-us-nuclear-energy-race.html?smid=nytcore-ios-share&referringSource=articleShare).\n\n:::\n\n:::: {.column width=\"30%\"}\n\n![](images/18/nytimes-qrcode.png)\n\n:::\n\n:::\n\n:::\n\n## Announcements {.smaller}\n\n-   Peer eval 2 due tonight at 11:59 pm\n\n-   ADD OTHER ANNOUNCEMENTS HERE\n\n# From last time\n\n## ae\\-12\\-penguins\\-model\\-multi {.smaller}\n\n::: appex\n-   Go to your ae project in RStudio.\n\n-   If you haven't yet done so, make sure all of your changes up to this point are committed and pushed, i.e., there's nothing left in your Git pane.\n\n-   If you haven't yet done so, click Pull to get today's application exercise file: *ae\\-12\\-penguins\\-model\\-multi\\.qmd*.\n\n-   Work through the application exercise in class, and render, commit, and push your edits.\n:::\n\n## What is the difference between $R^2$ and adjusted $R^2$? {.smaller}\n\n::: incremental\n\n- $R^2$:\n  - Proportion of variability in the outcome explained by the model.\n  - Useful for quantifying the fit of a given model.\n\n- Adjusted $R^2$:\n  - Proportion of variability in the outcome explained by the model, with a penalty added for the number of predictors in the model.\n  - Useful for comparing models.\n\n:::\n\n## Using adjusted $R^2$ to compare models {.smaller}\n\n::: incremental\n\n- Fit two (or more) models to the same data set with different sets of predictors, selected based on subject-matter knowledge and/or exploratory data analysis results, compared their adjusted $R^2$ values, and selected the model with the highest adjusted $R^2$ value.\n\n- Perform stepwise selection (forward or backward) to select a model that maximizes adjusted $R^2$:\n  - Backward elimination: Start with a model that includes all candidate predictors, and iteratively remove the predictor that results in the largest increase in adjusted $R^2$ until no further improvement is observed.\n  - Forward selection: Start with a model that includes no predictors, and iteratively add the predictor that results in the largest increase in adjusted $R^2$ until no further improvement is observed.\n\n:::\n\n## Using other criteria to compare models {.smaller}\n\nSame model selection approaches (subject-matter knowledge, stepwise selection) can be used with various model comparison criteria, such as:\n\n- Maximize adjusted $R^2$\n- _Minimize Akaike Information Criterion (AIC)_\n- _Minimize Bayesian Information Criterion (BIC)_\n- ...\n\n::: {.callout-note .fragment}\nOnly mentioning criteria other than adjusted $R^2$ here to build awareness that other criteria exist. We will not cover all of them in detail in this course (and this is not even a complete list), but you will encounter them in higher-level modeling courses and in practice.\n:::\n\n## Balancing fit and complexity\n\n::: incremental\n\n- More complex models (i.e., models with more predictors) tend to fit the data at hand better, but may not generalize well to new data.\n\n- Model selection criteria, like adjusted $R^2$, help balance model fit and complexity to avoid overfitting by penalizing models with more predictors.\n\n:::\n\n## Overfitting {.smaller}\n\n::: incremental\n\n- Overfitting occurs when a model captures not only the underlying relationship between predictors and outcome but also the random noise in the data.\n\n- Overfitted models tend to perform well on the observed data but poorly on new, unseen data.\n  - Good news: We have techniques to detect and prevent overfitting.\n  - Bad news: We won't get into those until next week.\n  - For today, we'll discuss one feature of data that can lead to overfitting -- outliers.\n\n:::\n\n# Outliers\n\n\n\n## Outliers in regression {.smaller}\n\n::: incremental\n- Outliers are observations that fall far from the main cloud of points.\n\n- They can be outlying in:\n  - the $x$ direction,\n  - the $y$ direction, or\n  - both.\n\n- However, being outlying in a univariate sense does **not** always mean being outlying from the **bivariate model**.\n\n- Points that are in-line with the bivariate model usually do **not** influence the least squares line, even if they are extreme in $x$, $y$, or both.\n:::\n\n## Participate ðŸ“±ðŸ’» {.xsmall}\n\n:::::: columns\n:::: {.column width=\"70%\" .small}\n::: wooclap\n\nWhich of the following best describes the circled point?\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](18-model-selection-overfitting_files/figure-revealjs/unnamed-chunk-2-1.png){width=768}\n:::\n:::\n\n\n::: wooclap-choices\n- It's an outlier in $x$ only\n- It's an outlier in $y$ only\n- It's an outlier in both $x$ and $y$ and influences the regression line\n- It's an outlier in both $x$ and $y$ but does not influence the regression line\n:::\n\n:::\n::::\n\n::: {.column width=\"30%\"}\n![](images/wooclap-qr.png){width=\"200\" fig-align=\"center\"}\n\n::: small\nScan the QR code or go to [app.wooclap.com/sta199](https://app.wooclap.com/sta199). Log in with your Duke NetID.\n:::\n\n:::\n::::::\n\n## Participate ðŸ“±ðŸ’» {.xsmall}\n\n:::::: columns\n:::: {.column width=\"70%\" .small}\n::: wooclap\n\nWhich of the following best describes the circled point?\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](18-model-selection-overfitting_files/figure-revealjs/unnamed-chunk-3-1.png){width=768}\n:::\n:::\n\n\n::: wooclap-choices\n- It's an outlier in $x$ only and influences the regression line\n- It's an outlier in $x$ only but does not influence the regression line\n- It's an outlier in $y$ only and influences the regression line\n- It's an outlier in $y$ only but does not influence the regression line\n:::\n\n:::\n::::\n\n::: {.column width=\"30%\"}\n![](images/wooclap-qr.png){width=\"200\" fig-align=\"center\"}\n\n::: small\nScan the QR code or go to [app.wooclap.com/sta199](https://app.wooclap.com/sta199). Log in with your Duke NetID.\n:::\n\n:::\n::::::\n\n## Types of outliers {.smaller}\n\n::: incremental\n- **Outliers:** Points or groups of points that stand out from the rest of the data.\n\n- **Leverage points:** Points that fall horizontally away from the center of the cloud tend to pull harder on the line, so we call them points with **high leverage** or **leverage points**.\n\n- **Influential points:** Outliers, generally high leverage points, that actually alter the slope or position of the regression line.\n  - We say a point is influential if omitting it would substantially change the regression model.\n:::\n\n## Practical advice\n\n::: incremental\n- Test your analysis **with and without** outliers.\n\n- Compare and discuss the impact of outliers on model fit.\n\n- Present both models to stakeholders to choose the most reasonable interpretation.\n:::\n\n. . .\n\n::: callout-warning\nRemoving outliers should only be done with strong justification -- excluding interesting or extreme cases can lead to misleading models, poor predictive performance, and flawed conclusions.\n:::\n\n## From the NYTimes {.smaller}\n\n[How China Raced Ahead of the U.S. on Nuclear Power](https://www.nytimes.com/interactive/2025/10/22/climate/china-us-nuclear-energy-race.html?smid=nytcore-ios-share&referringSource=articleShare)\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](18-model-selection-overfitting_files/figure-revealjs/unnamed-chunk-4-1.png){width=960}\n:::\n:::\n\n\n## US without outliers\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](18-model-selection-overfitting_files/figure-revealjs/unnamed-chunk-5-1.png){width=960}\n:::\n:::\n\n\n## China without outliers\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](18-model-selection-overfitting_files/figure-revealjs/unnamed-chunk-6-1.png){width=960}\n:::\n:::\n",
    "supporting": [
      "18-model-selection-overfitting_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}